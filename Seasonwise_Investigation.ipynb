{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-M_rHR3pgJgu"
   },
   "source": [
    "# **Objectives and Hypotheses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic2x8GRggQxg"
   },
   "source": [
    "## **Primary Hypotheses:**\n",
    "### Season-wise Hypothesis\n",
    "Can season-specific models better capture trends in bike demand than a unified model?\n",
    "\n",
    "## **Experimental Design:**\n",
    "### Baseline Model:\n",
    "XGBoost Regressor\n",
    "### Target Variable:\n",
    "Rented Bike Count\n",
    "### Dataset:\n",
    "Seoul Bike Sharing Demand\n",
    "### Data Split:\n",
    "80% training, 20% testing\n",
    "\n",
    "### Visualization:\n",
    "PCA for dimensionality reduction and cluster visualization\n",
    "\n",
    "### Performance Metrics:\n",
    "RMSE, MAE, and R²\n",
    "\n",
    "## **Expected Outcomes:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFkCZCB2WsRp"
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HsjQHFg_0Yz",
    "outputId": "0381b12f-74d3-4c89-cfd0-1316d5e81cc3"
   },
   "outputs": [],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nfbZTdB_w0K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "mpl.rcParams['axes.edgecolor'] = '#333333'\n",
    "mpl.rcParams['axes.linewidth'] = 0.8\n",
    "mpl.rcParams['xtick.color'] = '#333333'\n",
    "mpl.rcParams['ytick.color'] = '#333333'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6hwTLKoW-d_"
   },
   "source": [
    "# **Import Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sGCzaCo_7EK"
   },
   "outputs": [],
   "source": [
    "# Fetch Seoul Bike Sharing Demand dataset from UCI ML Repository\n",
    "seoul_bike_sharing_demand = fetch_ucirepo(id=560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2TCiohL_-VU"
   },
   "outputs": [],
   "source": [
    "# Data (as pandas dataframes)\n",
    "X_original = seoul_bike_sharing_demand.data.features\n",
    "y_original = seoul_bike_sharing_demand.data.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kceoOc1_Cgpp",
    "outputId": "0cab3c4a-da7a-466b-bbbe-6cc34c0e864d"
   },
   "outputs": [],
   "source": [
    "# Print dataset information\n",
    "print(\"Dataset Metadata:\")\n",
    "print(seoul_bike_sharing_demand.metadata)\n",
    "print(\"\\nVariable Information:\")\n",
    "print(seoul_bike_sharing_demand.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXhCwGVOBWIE",
    "outputId": "54c641ae-6576-42b2-cd35-cb4c56a07089"
   },
   "outputs": [],
   "source": [
    "# Examine feature information\n",
    "print(\"\\nOriginal feature columns:\")\n",
    "print(X_original.columns.tolist())\n",
    "print(\"\\nOriginal target variable:\")\n",
    "print(y_original.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhRPHn3wXK2V"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wY0aMkBoBtrX"
   },
   "outputs": [],
   "source": [
    "# Make 'Rented Bike Count' the new target if it exists\n",
    "\n",
    "if 'Rented Bike Count' in X_original.columns:\n",
    "    # Make 'Rented Bike Count' the new target\n",
    "    y = X_original[['Rented Bike Count']]\n",
    "    # Remove 'Rented Bike Count' from features\n",
    "    X = X_original.drop('Rented Bike Count', axis=1)\n",
    "    # Add original target to features\n",
    "    X = pd.concat([X, y_original], axis=1)\n",
    "\n",
    "else:\n",
    "    # If 'Rented Bike Count' is already the target, just confirm\n",
    "    print(\"'Rented Bike Count' is already the target variable.\")\n",
    "    y = y_original\n",
    "    X = X_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpuLenjDAB_4",
    "outputId": "5aa10a1a-b8a6-436a-bd99-73bfe3ed4407"
   },
   "outputs": [],
   "source": [
    "print(\"\\nNew feature columns:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"\\nNew target variable:\")\n",
    "print(y.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELZ5f9cmAE0W",
    "outputId": "95d2874b-8e8d-4751-d7c2-57825f5d8884"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in features:\")\n",
    "print(X.isnull().sum())\n",
    "print(\"\\nMissing values in target:\")\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataset to investigate periodic data\n",
    "X_viz = X.copy()\n",
    "y_viz = y.copy()\n",
    "\n",
    "seasonal_data = X_viz.copy()\n",
    "seasonal_data['Rented_Bike_Count'] = y_viz['Rented Bike Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal Bike Demand Analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Seasonal Bike Sharing Demand Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Prepare data\n",
    "seasonal_data = X_viz.copy()\n",
    "seasonal_data['Rented_Bike_Count'] = y_viz['Rented Bike Count']\n",
    "\n",
    "# Define season order\n",
    "season_order = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "season_colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']  # Blue, Green, Orange, Red\n",
    "\n",
    "# 1. Average Daily Demand by Season\n",
    "seasonal_avg = seasonal_data.groupby('Seasons')['Rented_Bike_Count'].mean().reindex(season_order)\n",
    "\n",
    "bars = ax1.bar(seasonal_avg.index, seasonal_avg.values, \n",
    "               color=season_colors, alpha=0.8, edgecolor='#333333', linewidth=1.2)\n",
    "ax1.set_xlabel('Season', fontweight='bold')\n",
    "ax1.set_ylabel('Average Daily Bike Rentals', fontweight='bold')\n",
    "ax1.set_title('Average Daily Demand by Season', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Box plot showing distribution of demand by season\n",
    "seasonal_data_ordered = []\n",
    "season_labels = []\n",
    "for season in season_order:\n",
    "    season_data = seasonal_data[seasonal_data['Seasons'] == season]['Rented_Bike_Count']\n",
    "    if len(season_data) > 0:\n",
    "        seasonal_data_ordered.append(season_data)\n",
    "        season_labels.append(season)\n",
    "\n",
    "box_plot = ax2.boxplot(seasonal_data_ordered, tick_labels=season_labels, patch_artist=True)\n",
    "for patch, color in zip(box_plot['boxes'], season_colors[:len(season_labels)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax2.set_xlabel('Season', fontweight='bold')\n",
    "ax2.set_ylabel('Daily Bike Rentals', fontweight='bold')\n",
    "ax2.set_title('Distribution of Daily Demand by Season', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print seasonal summary statistics\n",
    "print(\"Seasonal Bike Sharing Analysis\")\n",
    "\n",
    "for season in season_order:\n",
    "    season_data = seasonal_data[seasonal_data['Seasons'] == season]['Rented_Bike_Count']\n",
    "    if len(season_data) > 0:\n",
    "        print(f\"\\n{season}:\")\n",
    "        print(f\"  Average daily rentals: {season_data.mean():.1f}\")\n",
    "        print(f\"  Peak daily demand: {season_data.max():,}\")\n",
    "        print(f\"  Lowest daily demand: {season_data.min():,}\")\n",
    "        print(f\"  Standard deviation: {season_data.std():.1f}\")\n",
    "        print(f\"  Days of data: {len(season_data):,}\")\n",
    "\n",
    "# Compare seasons\n",
    "print(f\"\\nSeasonal Comparisons:\")\n",
    "\n",
    "# Find best and worst seasons\n",
    "season_averages = {}\n",
    "for season in season_order:\n",
    "    season_data = seasonal_data[seasonal_data['Seasons'] == season]['Rented_Bike_Count']\n",
    "    if len(season_data) > 0:\n",
    "        season_averages[season] = season_data.mean()\n",
    "\n",
    "if season_averages:\n",
    "    best_season = max(season_averages, key=season_averages.get)\n",
    "    worst_season = min(season_averages, key=season_averages.get)\n",
    "    \n",
    "    print(f\"Highest demand season: {best_season} ({season_averages[best_season]:.1f} avg daily)\")\n",
    "    print(f\"Lowest demand season: {worst_season} ({season_averages[worst_season]:.1f} avg daily)\")\n",
    "    \n",
    "    # Calculate difference\n",
    "    seasonal_range = season_averages[best_season] - season_averages[worst_season]\n",
    "    seasonal_range_pct = (seasonal_range / season_averages[worst_season]) * 100\n",
    "    \n",
    "    print(f\"Seasonal variation: {seasonal_range:.1f} rentals ({seasonal_range_pct:.1f}% increase from lowest to highest)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Bike Demand Over the Year\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Prepare data\n",
    "daily_data = X_viz.copy()\n",
    "daily_data['Rented_Bike_Count'] = y_viz['Rented Bike Count']\n",
    "\n",
    "# Convert Date to datetime if needed\n",
    "if daily_data['Date'].dtype == 'object':\n",
    "    daily_data['Date'] = pd.to_datetime(daily_data['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Group by date and sum hourly data to get daily totals\n",
    "daily_totals = daily_data.groupby('Date')['Rented_Bike_Count'].sum().reset_index()\n",
    "\n",
    "# Plot daily demand\n",
    "ax.plot(daily_totals['Date'], daily_totals['Rented_Bike_Count'], \n",
    "        color='#1f77b4', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Add a 7-day rolling average to smooth the trend\n",
    "if len(daily_totals) > 7:\n",
    "    rolling_avg = daily_totals['Rented_Bike_Count'].rolling(window=7, center=True).mean()\n",
    "    ax.plot(daily_totals['Date'], rolling_avg, \n",
    "            color='#ff7f0e', linewidth=2.5, label='7-Day Moving Average')\n",
    "    ax.legend()\n",
    "\n",
    "ax.set_xlabel('Date', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Daily Bike Rentals', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Daily Bike Demand Over the Year', fontweight='bold', fontsize=16)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Format x-axis to show months nicely\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical Encoding\n",
    "For ML processes we often want to encode data for more efficient processing, however in this case, we need to choose an encoding which preserves the periodic patterns that will help us make our demand predictions. We can use a technique called Cyclical Encoding for the features in the dataset which are periodic in nature.  This will better help us capture seasonal, monthly, weekly and diurnal trends for our demand prediction.\n",
    "\n",
    "The technique is borrowed from harmonic analysis and signal processing, where we place our periodic data on a unite circle rather than on a linear scale.  This helps us preserve the natural relationships, for example, between Sunday and Monday, Decemeber and Janaury, which are now neighbors rather than distance points.\n",
    "\n",
    "This also helps smooth artificial jumps between time periods, for example between the first day of January and the last day of December, or between the end of Spring and the start of Summer.  The elegance here is that Euclidean distance in sin/cos space matches circular distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cyclical encoding for months and plot the difference\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Changed from (2, 3) to (1, 3)\n",
    "fig.suptitle('Cyclical Encoding Example: From Linear to Circular Representation', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Months of the year\n",
    "months = np.arange(1, 13)  # Jan=1 to Dec=12\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Linear representation\n",
    "axes[0].scatter(months, [0]*12, c=range(12), cmap='viridis', s=100)  # Changed from axes[0, 0] to axes[0]\n",
    "for i, name in enumerate(month_names):\n",
    "    axes[0].annotate(name, (months[i], 0), xytext=(0, 20), \n",
    "                       textcoords='offset points', ha='center')\n",
    "axes[0].set_xlabel('Month (Linear)', fontweight='bold')\n",
    "axes[0].set_title('Jan and Dec plotted linearly', fontweight='bold')\n",
    "axes[0].set_ylim(-0.5, 1)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cyclical representation using sin/cos\n",
    "month_angles = 2 * np.pi * months / 12\n",
    "month_sin = np.sin(month_angles)\n",
    "month_cos = np.cos(month_angles)\n",
    "\n",
    "# Plot on unit circle\n",
    "circle = plt.Circle((0, 0), 1, fill=False, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].add_patch(circle)  # Changed from axes[0, 1] to axes[1]\n",
    "axes[1].scatter(month_cos, month_sin, c=range(12), cmap='viridis', s=100)\n",
    "for i, name in enumerate(month_names):\n",
    "    axes[1].annotate(name, (month_cos[i], month_sin[i]), xytext=(5, 5), \n",
    "                       textcoords='offset points', fontsize=8)\n",
    "axes[1].set_xlabel('Month Cosine', fontweight='bold')\n",
    "axes[1].set_ylabel('Month Sine', fontweight='bold')\n",
    "axes[1].set_title('Jan and Dec are neighbors on the unit circle', fontweight='bold')\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Encoding formulas\n",
    "axes[2].text(0.1, 0.8, 'Cyclical Encoding Formulas:', fontsize=14, fontweight='bold', transform=axes[2].transAxes)  # Changed from axes[0, 2] to axes[2]\n",
    "axes[2].text(0.1, 0.7, 'For month M (1-12):', fontsize=12, transform=axes[2].transAxes)\n",
    "axes[2].text(0.1, 0.6, 'angle = 2π × M / 12', fontsize=12, transform=axes[2].transAxes, family='monospace')\n",
    "axes[2].text(0.1, 0.5, 'month_sin = sin(angle)', fontsize=12, transform=axes[2].transAxes, family='monospace')\n",
    "axes[2].text(0.1, 0.4, 'month_cos = cos(angle)', fontsize=12, transform=axes[2].transAxes, family='monospace')\n",
    "axes[2].text(0.1, 0.25, 'Benefits:', fontsize=12, fontweight='bold', transform=axes[2].transAxes)\n",
    "axes[2].text(0.1, 0.15, '• Preserves cyclical relationships', fontsize=10, transform=axes[2].transAxes)\n",
    "axes[2].text(0.1, 0.1, '• Smooth transitions', fontsize=10, transform=axes[2].transAxes)\n",
    "axes[2].text(0.1, 0.05, '• No arbitrary ordering', fontsize=10, transform=axes[2].transAxes)\n",
    "axes[2].set_xlim(0, 1)\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert periodic data to cyclical encoding\n",
    "def create_cyclical_encoding(X_data):\n",
    "    X_cyclical = X_data.copy()\n",
    "    \n",
    "    X_cyclical['Date'] = pd.to_datetime(X_cyclical['Date'])\n",
    "    \n",
    "    X_cyclical['Month_sin'] = np.sin(2 * np.pi * X_cyclical['Month'] / 12)\n",
    "    X_cyclical['Month_cos'] = np.cos(2 * np.pi * X_cyclical['Month'] / 12)\n",
    "    \n",
    "    X_cyclical['DayOfYear'] = X_cyclical['Date'].dt.dayofyear\n",
    "    X_cyclical['DayOfYear_sin'] = np.sin(2 * np.pi * X_cyclical['DayOfYear'] / 365)\n",
    "    X_cyclical['DayOfYear_cos'] = np.cos(2 * np.pi * X_cyclical['DayOfYear'] / 365)\n",
    "    \n",
    "    X_cyclical['Hour_sin'] = np.sin(2 * np.pi * X_cyclical['Hour'] / 24)\n",
    "    X_cyclical['Hour_cos'] = np.cos(2 * np.pi * X_cyclical['Hour'] / 24)\n",
    "    \n",
    "    X_cyclical['DayOfWeek_sin'] = np.sin(2 * np.pi * X_cyclical['DayOfWeek'] / 7)\n",
    "    X_cyclical['DayOfWeek_cos'] = np.cos(2 * np.pi * X_cyclical['DayOfWeek'] / 7)\n",
    "    \n",
    "    # Map seasons to numbers (0-3) in seasonal order\n",
    "    season_mapping = {\n",
    "        'Winter': 0,    # Start of cycle\n",
    "        'Spring': 1,    # 1/4 through cycle  \n",
    "        'Summer': 2,    # 1/2 through cycle\n",
    "        'Autumn': 3     # 3/4 through cycle\n",
    "    }\n",
    "    \n",
    "    # Create numeric season column\n",
    "    X_cyclical['Season_numeric'] = X_cyclical['Seasons'].map(season_mapping)\n",
    "    \n",
    "    # Create seasonal cyclical encoding\n",
    "    X_cyclical['Season_sin'] = np.sin(2 * np.pi * X_cyclical['Season_numeric'] / 4)\n",
    "    X_cyclical['Season_cos'] = np.cos(2 * np.pi * X_cyclical['Season_numeric'] / 4)    \n",
    "    \n",
    "    return X_cyclical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reset to avoid returning to the start of the notebook\n",
    "seoul_bike_sharing_demand = fetch_ucirepo(id=560)\n",
    "X_original = seoul_bike_sharing_demand.data.features\n",
    "y_original = seoul_bike_sharing_demand.data.targets\n",
    "\n",
    "# Make 'Rented Bike Count' the new target if it exists\n",
    "\n",
    "if 'Rented Bike Count' in X_original.columns:\n",
    "    # Make 'Rented Bike Count' the new target\n",
    "    y = X_original[['Rented Bike Count']]\n",
    "    # Remove 'Rented Bike Count' from features\n",
    "    X = X_original.drop('Rented Bike Count', axis=1)\n",
    "    # Add original target to features\n",
    "    X = pd.concat([X, y_original], axis=1)\n",
    "\n",
    "else:\n",
    "    # If 'Rented Bike Count' is already the target, just confirm\n",
    "    print(\"'Rented Bike Count' is already the target variable.\")\n",
    "    y = y_original\n",
    "    X = X_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0ZypEjpAjqO"
   },
   "outputs": [],
   "source": [
    "# 1. Convert date column to datetime and extract useful components. date format is DD/MM/YYYY\n",
    "if 'Date' in X.columns:\n",
    "    # Specify the correct date format as DD/MM/YYYY\n",
    "    X['Date'] = pd.to_datetime(X['Date'], format='%d/%m/%Y')\n",
    "    X['Year'] = X['Date'].dt.year\n",
    "    X['Month'] = X['Date'].dt.month\n",
    "    X['Day'] = X['Date'].dt.day\n",
    "    X['DayOfWeek'] = X['Date'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create cyclical encoding before adding one-encoding\n",
    "X = create_cyclical_encoding(X)\n",
    "\n",
    "# Drop Date after cyclical encoding\n",
    "X = X.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "KyW3-YG4QpIi",
    "outputId": "f77d5bc5-2961-4612-b0ee-d04a297595d4"
   },
   "outputs": [],
   "source": [
    "# 3. Convert categorical features to numeric using one-hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUXcIclqElR5"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "id": "fMNS9RuVTF7w",
    "outputId": "1f6e6fda-0d94-4e95-a53e-976f0aa0e5ba"
   },
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "\n",
    "correlation_matrix = X_train_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Use the calculated correlation matrix in the heatmap\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, fmt='.2f')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual removal of collinear features, since cyclical features will have intentionally high VIF\n",
    "definitely_remove = [\"DayOfYear\", \"Month\", \"Day\", \"Dew point temperature\"]  \n",
    "X_train_filtered = X_train.drop(columns=[f for f in definitely_remove if f in X_train.columns])\n",
    "X_test_filtered = X_test.drop(columns=[f for f in definitely_remove if f in X_test.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8x5rCrwUSxP"
   },
   "outputs": [],
   "source": [
    "# Standardize the filtered features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
    "X_test_scaled = scaler.transform(X_test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfs7TqaNXbmM"
   },
   "source": [
    "# **Model Development**\n",
    "We break development into the following steps:\n",
    "1. Create a baseline XGBoost model using default parameters\n",
    "2. Use hyperparameter tuning with regularization to optimize the model\n",
    "3. Implement early stopping to prevent overfitting\n",
    "4. Evaluate the results and create visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Reset\n",
    "Before proceeding, let's create a quick way to start with fresh preprocessed data defined by our earlier scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    \"\"\"\n",
    "    Minimal preprocessing pipeline for bike sharing demand data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    seoul_bike_sharing_demand = fetch_ucirepo(id=560)\n",
    "    X_original = seoul_bike_sharing_demand.data.features\n",
    "    y_original = seoul_bike_sharing_demand.data.targets\n",
    "    \n",
    "    # Handle target variable\n",
    "    if 'Rented Bike Count' in X_original.columns:\n",
    "        y = X_original[['Rented Bike Count']]\n",
    "        X = X_original.drop('Rented Bike Count', axis=1)\n",
    "        X = pd.concat([X, y_original], axis=1)\n",
    "    else:\n",
    "        y = y_original\n",
    "        X = X_original\n",
    "        \n",
    "    # Process dates and create cyclical features\n",
    "    if 'Date' in X.columns:\n",
    "        X['Date'] = pd.to_datetime(X['Date'], format='%d/%m/%Y')\n",
    "        X['Year'] = X['Date'].dt.year\n",
    "        X['Month'] = X['Date'].dt.month\n",
    "        X['Day'] = X['Date'].dt.day\n",
    "        X['DayOfWeek'] = X['Date'].dt.dayofweek\n",
    "        \n",
    "    # Create cyclical encoding and clean up\n",
    "    X = create_cyclical_encoding(X)\n",
    "    X = X.drop('Date', axis=1)\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Remove collinear features\n",
    "    definitely_remove = [\"DayOfYear\", \"Month\", \"Day\", \"Dew point temperature\"]\n",
    "    actually_removed = [f for f in definitely_remove if f in X_train.columns]\n",
    "    \n",
    "    if actually_removed:\n",
    "        X_train = X_train.drop(columns=actually_removed)\n",
    "        X_test = X_test.drop(columns=actually_removed)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, X_train.columns.tolist()\n",
    "\n",
    "# Run preprocessing\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_names = preprocess_data()\n",
    "print(f\"Data preprocessed: {len(feature_names)} features, {len(X_train_scaled)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Baseline Model and Check for Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline XGBoost model with minimal configuration\n",
    "baseline_model = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "# Make predictions\n",
    "baseline_train_preds = baseline_model.predict(X_train_scaled)\n",
    "baseline_test_preds = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_metrics = {\n",
    "    'train_rmse': math.sqrt(mean_squared_error(y_train, baseline_train_preds)),\n",
    "    'train_r2': r2_score(y_train, baseline_train_preds),\n",
    "    'test_rmse': math.sqrt(mean_squared_error(y_test, baseline_test_preds)),\n",
    "    'test_r2': r2_score(y_test, baseline_test_preds)\n",
    "}\n",
    "\n",
    "print(f\"Training RMSE:  {baseline_metrics['train_rmse']:.2f}\")\n",
    "print(f\"Training R²:    {baseline_metrics['train_r2']:.4f}\")\n",
    "print(f\"Test RMSE:      {baseline_metrics['test_rmse']:.2f}\")\n",
    "print(f\"Test R²:        {baseline_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# Check for overfitting in baseline\n",
    "rmse_gap = baseline_metrics['test_rmse'] - baseline_metrics['train_rmse']\n",
    "print(f\"\\nOverfitting Check:\")\n",
    "print(f\"RMSE Gap (Test - Train): {rmse_gap:.2f}\")\n",
    "if rmse_gap < 30:\n",
    "    print(\"Minimal overfitting detected\")\n",
    "elif rmse_gap < 60:\n",
    "    print(\"Moderate overfitting detected\")\n",
    "else:\n",
    "    print(\"Significant overfitting detected\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the baseline model overfits our training data with a RMSE Gap more than 60, we leave that for now and address overfitting for both the baseline and tuned models in step three.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform Hyperparameter Tuning\n",
    "Here we set up and conduct hyperparameter tuning to identify the optimal values for predicting bike usage. \n",
    "\n",
    "A table of the key XGBoost hyperparameters that we will use to control different aspects of model training, is provided:\n",
    "| Parameter | Function | Impact |\n",
    "|:-----------|:----------|:--------|\n",
    "| `n_estimators` | Number of decision trees (rounds) | More trees = better fit |\n",
    "| `learning_rate` | Step size for each tree's update | Lower = slower, more stable |\n",
    "| `max_depth` | Maximum depth of each tree | Higher = more complex trees |\n",
    "| `min_child_weight` | Minimum samples in leaf nodes | Higher = prevents overfitting |\n",
    "| `reg_alpha` | L1 regularization (Lasso) | Higher = simpler model |\n",
    "| `reg_lambda` | L2 regularization (Ridge) | Higher = smoother predictions |\n",
    "| `subsample` | Fraction of samples per tree | Lower = more randomization |\n",
    "| `colsample_bytree` | Fraction of features per tree | Lower = reduces correlation |\n",
    "\n",
    "We can use gridsearch to evaluate different combinations of these parameters when we build our XGBoost decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive parameter grid for tuned model with regularization focus\n",
    "param_grid_comprehensive = {\n",
    "    # Number of boosting rounds\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    \n",
    "    # Learning rate - conservative values to prevent overfitting\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    \n",
    "    # Tree structure - moderate depth to capture patterns without overfitting\n",
    "    'max_depth': [3, 4, 6],\n",
    "    \n",
    "    # Minimum child weight - higher values prevent overfitting\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    \n",
    "    # L1 regularization - encourages sparsity\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    \n",
    "    # L2 regularization - smooths predictions\n",
    "    'reg_lambda': [1, 1.5, 2],\n",
    "    \n",
    "    # Subsampling - reduces overfitting through randomization\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    \n",
    "    # Feature subsampling - reduces correlation between trees\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early Stopping with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we address overfitting with the baseline model\n",
    "\n",
    "We can adjust our baseline model by adding hyperparameters, first we try adjusting the learning rate and tree depth:\n",
    "| Parameter | Function | Impact |\n",
    "|:-----------|:----------|:--------|\n",
    "| `n_estimators` | Number of decision trees (rounds) | More trees = better fit |\n",
    "| `learning_rate` | Step size for each tree's update | Lower = slower, more stable |\n",
    "| `max_depth` | Maximum depth of each tree | Higher = more complex trees |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple baseline with basic settings\n",
    "baseline_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "baseline_model.fit(X_train_scaled, y_train.values.ravel())\n",
    "baseline_train_preds = baseline_model.predict(X_train_scaled)\n",
    "baseline_test_preds = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_train_rmse = math.sqrt(mean_squared_error(y_train, baseline_train_preds))\n",
    "baseline_test_rmse = math.sqrt(mean_squared_error(y_test, baseline_test_preds))\n",
    "baseline_train_r2 = r2_score(y_train, baseline_train_preds)\n",
    "baseline_test_r2 = r2_score(y_test, baseline_test_preds)\n",
    "\n",
    "print(f\"BASELINE PERFORMANCE:\")\n",
    "print(f\"Train RMSE: {baseline_train_rmse:.2f} | Test RMSE: {baseline_test_rmse:.2f}\")\n",
    "print(f\"Train R²:   {baseline_train_r2:.4f} | Test R²:   {baseline_test_r2:.4f}\")\n",
    "print(f\"Overfitting Gap: {baseline_test_rmse:.2f} -  {baseline_train_rmse:.2f} = {baseline_test_rmse - baseline_train_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improved our overfitting reducing the gap from 91.59 to 55.37 in this new model with minimal regularization.  This model has moderate overfitting but is a huge improvement.  Still, we can do better by adding stronger regularization.\n",
    "| Parameter | Function | Impact |\n",
    "|:-----------|:----------|:--------|\n",
    "| `n_estimators` | Number of decision trees (rounds) | More trees = better fit |\n",
    "| `learning_rate` | Step size for each tree's update | Lower = slower, more stable |\n",
    "| `max_depth` | Maximum depth of each tree | Higher = more complex trees |\n",
    "| `min_child_weight` | Minimum samples in leaf nodes | Higher = prevents overfitting |\n",
    "| `reg_alpha` | L1 regularization (Lasso) | Higher = simpler model |\n",
    "| `reg_lambda` | L2 regularization (Ridge) | Higher = smoother predictions |\n",
    "| `subsample` | Fraction of samples per tree | Lower = more randomization |\n",
    "| `colsample_bytree` | Fraction of features per tree | Lower = reduces correlation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced baseline with regularization to address overfitting\n",
    "baseline_regularized = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.08,          # Slightly lower\n",
    "    max_depth=5,                 # Reduced depth\n",
    "    min_child_weight=3,          # Higher minimum\n",
    "    reg_alpha=0.5,               # L1 regularization\n",
    "    reg_lambda=2,                # L2 regularization\n",
    "    subsample=0.8,               # Sample 80% of data\n",
    "    colsample_bytree=0.8,        # Sample 80% of features\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "baseline_regularized.fit(X_train_scaled, y_train.values.ravel())\n",
    "reg_train_preds = baseline_regularized.predict(X_train_scaled)\n",
    "reg_test_preds = baseline_regularized.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "reg_train_rmse = math.sqrt(mean_squared_error(y_train, reg_train_preds))\n",
    "reg_test_rmse = math.sqrt(mean_squared_error(y_test, reg_test_preds))\n",
    "reg_train_r2 = r2_score(y_train, reg_train_preds)\n",
    "reg_test_r2 = r2_score(y_test, reg_test_preds)\n",
    "\n",
    "print(f\"REGULARIZED BASELINE PERFORMANCE:\")\n",
    "print(f\"Train RMSE: {reg_train_rmse:.2f} | Test RMSE: {reg_test_rmse:.2f}\")\n",
    "print(f\"Train R²:   {reg_train_r2:.4f} | Test R²:   {reg_test_r2:.4f}\")\n",
    "print(f\"Overfitting Gap: {reg_test_rmse - reg_train_rmse:.2f}\")\n",
    "\n",
    "print(f\"\\nBASELINE IMPROVEMENT:\")\n",
    "print(f\"Gap Reduction: {(baseline_test_rmse - baseline_train_rmse) - (reg_test_rmse - reg_train_rmse):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this new baseline model as it shows a significant improvement in generalization (34.38 is a much lower overfitting gap). . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a seasonally-aware model while staying vigiliant of overfitting\n",
    "To capture the complex seasonal interactions, we now turn to Grid Search to try to create a more seasonally-aware model. Our max depth parameter helps us capture complex seasonal interactions, reg_alpha and reg_lambda balance feature importance so cyclical features aren't overshadowed.  We adjust our learning rate so we can learn seasonal patterns slowly, and employ early stopping to ensure that the seasonally-aware model does not overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti-overfitting parameter grid specifically designed for seasonal patterns\n",
    "param_grid_seasonal = {\n",
    "    # Conservative number of trees\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    \n",
    "    # Lower learning rates for stable seasonal learning\n",
    "    'learning_rate': [0.01, 0.05, 0.08],\n",
    "    \n",
    "    # Moderate tree depths to capture seasonal interactions without overfitting\n",
    "    'max_depth': [4, 5, 6],\n",
    "    \n",
    "    # Higher minimum child weight for generalization\n",
    "    'min_child_weight': [3, 5, 7],\n",
    "    \n",
    "    # Strong L1 regularization for feature selection (helps cyclical features shine)\n",
    "    'reg_alpha': [0.5, 1.0, 2.0],\n",
    "    \n",
    "    # Strong L2 regularization for smooth predictions\n",
    "    'reg_lambda': [2, 3, 4],\n",
    "    \n",
    "    # Aggressive subsampling for randomization (prevents seasonal overfitting)\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8]\n",
    "}\n",
    "\n",
    "print(f\"Parameter combinations: {np.prod([len(v) for v in param_grid_seasonal.values()])}\")\n",
    "print(\"Focus: Optimize cyclical feature usage while preventing overfitting\")\n",
    "\n",
    "# Time Series Cross-Validation (preserves temporal relationships)\n",
    "tscv = TimeSeriesSplit(n_splits=5, test_size=len(X_train_scaled)//8)\n",
    "print(\"Using TimeSeriesSplit to respect temporal order in bike demand data\")\n",
    "\n",
    "# XGBoost configured for seasonal data\n",
    "xgb_seasonal = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0,\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "# Grid Search with seasonal focus\n",
    "grid_search_seasonal = GridSearchCV(\n",
    "    estimator=xgb_seasonal,\n",
    "    param_grid=param_grid_seasonal,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=7,   # number of cores to use (adjust for your system)\n",
    "    verbose=1,  # 0-suppress progress, 1-show minimal progress, 2-show progress with timing, 3- detailed progress\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Starting seasonal-aware grid search...\")\n",
    "print(\"This will find optimal parameters for cyclical feature utilization, and will take some time to finish.\")\n",
    "print(\"Adjusting n_jobs can speed up the process based on the number of cores available.  Verbose progress is turned off, but feel free to adjust.\")\n",
    "# Fit the grid search\n",
    "grid_search_seasonal.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "print(\"Seasonal grid search completed!\")\n",
    "\n",
    "# Extract best results\n",
    "best_params_seasonal = grid_search_seasonal.best_params_\n",
    "best_cv_score = grid_search_seasonal.best_score_\n",
    "\n",
    "print(f\"\\nBEST SEASONAL PARAMETERS:\")\n",
    "print(\"-\" * 35)\n",
    "for param, value in best_params_seasonal.items():\n",
    "    print(f\"{param:18s}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV Score (Neg MSE): {best_cv_score:.2f}\")\n",
    "print(f\"Best CV RMSE: {math.sqrt(-best_cv_score):.2f}\")\n",
    "\n",
    "# ================================================================================================\n",
    "# FINAL MODEL WITH EARLY STOPPING\n",
    "# ================================================================================================\n",
    "\n",
    "print(f\"\\nTRAINING FINAL SEASONAL MODEL WITH EARLY STOPPING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create validation split for early stopping\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train_scaled, y_train.values.ravel(), test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training split: {X_train_final.shape}\")\n",
    "print(f\"Validation split: {X_val_final.shape}\")\n",
    "\n",
    "# Final seasonal model with best parameters + early stopping\n",
    "final_model_balanced = xgb.XGBRegressor(\n",
    "    n_estimators=280,           # More trees = higher overfitting risk (was 320, tried 250, 200, 150)\n",
    "    learning_rate=0.045,        # Moderate (was 0.05, tried 0.08, 0.03)\n",
    "    max_depth=6,                # Controls amount of complexity for cyclical features (was 5, tried 4)  \n",
    "    min_child_weight=3,         # Controls amount of complexity for cyclical features (was 2.5, tried 7)\n",
    "    reg_alpha=1.0,              # Controls overfitting (was 0.9, tried 0.7, 0.5, 2.5)  \n",
    "    reg_lambda=3.2,             # Controls overfitting (was 3.2, tried 2.0, 4.0)\n",
    "    subsample=0.82,             # Less aggressive (was 0.85, tried 0.8, 0.7, 0.6)\n",
    "    colsample_bytree=0.8,       # Helps cyclical features (was 0.85, tried 0.7, 0.6)\n",
    "    early_stopping_rounds=18,   # More patient (was 20, tried 25, 15, 8)\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train with early stopping validation\n",
    "print(\"Training seasonal model with early stopping...\")\n",
    "final_model_balanced.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    eval_set=[(X_val_final, y_val_final)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Seasonal model training completed!\")\n",
    "\n",
    "# Check early stopping results\n",
    "if hasattr(final_model_balanced, 'best_iteration') and final_model_balanced.best_iteration:\n",
    "    print(f\"Early stopping triggered at iteration: {final_model_balanced.best_iteration}\")\n",
    "    trees_saved = 280 - final_model_balanced.best_iteration\n",
    "    print(f\"Trees saved from overfitting: {trees_saved}\")\n",
    "else:\n",
    "    print(f\"Used all 280 trees (early stopping didn't trigger)\")\n",
    "\n",
    "# Evaluate final model - FIX THESE LINES:\n",
    "balanced_train_preds = final_model_balanced.predict(X_train_scaled)\n",
    "balanced_test_preds = final_model_balanced.predict(X_test_scaled)\n",
    "\n",
    "balanced_train_rmse = math.sqrt(mean_squared_error(y_train, balanced_train_preds))\n",
    "balanced_test_rmse = math.sqrt(mean_squared_error(y_test, balanced_test_preds))\n",
    "balanced_train_r2 = r2_score(y_train, balanced_train_preds)\n",
    "balanced_test_r2 = r2_score(y_test, balanced_test_preds)\n",
    "\n",
    "print(f\"\\nFINAL BALANCED MODEL PERFORMANCE:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Training RMSE:  {balanced_train_rmse:.2f}\")\n",
    "print(f\"Training R²:    {balanced_train_r2:.4f}\")\n",
    "print(f\"Test RMSE:      {balanced_test_rmse:.2f}\")\n",
    "print(f\"Test R²:        {balanced_test_r2:.4f}\")\n",
    "\n",
    "# Overfitting analysis\n",
    "balanced_gap = balanced_test_rmse - balanced_train_rmse\n",
    "print(f\"\\nOverfitting Gap: {balanced_gap:.2f}\")\n",
    "\n",
    "if balanced_gap < 30:\n",
    "    print(\"Excellent generalization achieved!\")\n",
    "elif balanced_gap < 50:\n",
    "    print(\"Good generalization achieved!\")\n",
    "else:\n",
    "    print(\"Some overfitting remains - but model still usable\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f\"\\nCYCLICAL FEATURE IMPORTANCE:\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': final_model_balanced.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Highlight cyclical features\n",
    "cyclical_features = feature_importance[feature_importance['feature'].str.contains('_sin|_cos')].head(8)\n",
    "\n",
    "if len(cyclical_features) > 0:\n",
    "    print(f\"Top cyclical features in final model:\")\n",
    "    for i, (_, row) in enumerate(cyclical_features.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['feature']:<20s}: {row['importance']:.4f}\")\n",
    "        \n",
    "    cyclical_importance_sum = cyclical_features['importance'].sum()\n",
    "    total_importance = feature_importance['importance'].sum()\n",
    "    cyclical_percentage = (cyclical_importance_sum / total_importance) * 100\n",
    "    \n",
    "    print(f\"\\nCyclical features account for {cyclical_percentage:.1f}% of total importance\")\n",
    "    print(\"Seasonal patterns successfully captured!\")\n",
    "else:\n",
    "    print(\"No cyclical features found in top importance - check feature engineering\")\n",
    "\n",
    "print(f\"\\nBALANCED MODEL READY FOR DEPLOYMENT!\")\n",
    "print(f\"The model successfully leverages cyclical encodings for bike demand prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjdW7iiSgAOi"
   },
   "source": [
    "## 4. Evaluation and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare combined data\n",
    "y_actual_all = np.concatenate([y_train.values.ravel(), y_test.values.ravel()])\n",
    "y_pred_all = np.concatenate([balanced_train_preds, balanced_test_preds])\n",
    "\n",
    "# Create date range (adjust start_date to match your data period)\n",
    "start_date = pd.Timestamp('2016-12-01')  # Adjust based on your actual data period\n",
    "dates = pd.date_range(start=start_date, periods=len(y_actual_all), freq='h')\n",
    "\n",
    "# Create DataFrame\n",
    "df_combined = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Actual': y_actual_all,\n",
    "    'Predicted': y_pred_all\n",
    "})\n",
    "\n",
    "# Aggregate to weekly\n",
    "df_weekly = df_combined.set_index('Date').resample('W').mean().reset_index()\n",
    "\n",
    "# Create the time series plot\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot actual and predicted lines\n",
    "ax.plot(df_weekly['Date'], df_weekly['Actual'], \n",
    "        color='#1f77b4', linewidth=2, alpha=0.8, label='Actual')\n",
    "ax.plot(df_weekly['Date'], df_weekly['Predicted'], \n",
    "        color='#ff4444', linewidth=2, alpha=0.8, label='Predictions')\n",
    "\n",
    "# Add train/test split line\n",
    "# Calculate split date based on train/test sizes\n",
    "train_size = len(y_train)\n",
    "total_size = len(y_actual_all)\n",
    "split_ratio = train_size / total_size\n",
    "\n",
    "# Find corresponding date in weekly data\n",
    "split_week_idx = int(len(df_weekly) * split_ratio)\n",
    "if split_week_idx < len(df_weekly):\n",
    "    split_date = df_weekly.iloc[split_week_idx]['Date']\n",
    "    ax.axvline(x=split_date, color='green', linestyle='--', alpha=0.7, linewidth=2,\n",
    "              label=f'Train/Test Split ({split_date.strftime(\"%Y-%m-%d\")})')\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Date', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Weekly Average Bikes Shared', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Weekly Bike Demand: Actual vs Predicted Over Time', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Legend and grid\n",
    "ax.legend(loc='upper right', frameon=True, fancybox=True, shadow=True, fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Format x-axis to show months\n",
    "from matplotlib.dates import DateFormatter, MonthLocator\n",
    "ax.xaxis.set_major_locator(MonthLocator())\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Weekly Time Series Statistics:\")\n",
    "print(f\"Total weeks: {len(df_weekly)}\")\n",
    "print(f\"Date range: {df_weekly['Date'].min().strftime('%Y-%m-%d')} to {df_weekly['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Weekly RMSE: {np.sqrt(((df_weekly['Actual'] - df_weekly['Predicted'])**2).mean()):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LaPtYSDfZDMt",
    "outputId": "839b0fe2-fa67-4372-aca8-0eb21e090742"
   },
   "outputs": [],
   "source": [
    "def analyze_seasonal_patterns(df_weekly):\n",
    "    \"\"\"\n",
    "    Analyze how well the model captures seasonal patterns\n",
    "    \"\"\"   \n",
    "    # Add seasonal information\n",
    "    df_analysis = df_weekly.copy()\n",
    "    df_analysis['Month'] = df_analysis['Date'].dt.month\n",
    "    df_analysis['Season'] = df_analysis['Month'].map({\n",
    "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "        9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "    })\n",
    "    \n",
    "    # Calculate seasonal averages\n",
    "    seasonal_comparison = df_analysis.groupby('Season')[['Actual', 'Predicted']].mean()\n",
    "    \n",
    "    print(\"Seasonal Performance:\")\n",
    "    for season in ['Spring', 'Summer', 'Autumn', 'Winter']:\n",
    "        if season in seasonal_comparison.index:\n",
    "            actual = seasonal_comparison.loc[season, 'Actual']\n",
    "            pred = seasonal_comparison.loc[season, 'Predicted']\n",
    "            error = abs(actual - pred)\n",
    "            error_pct = (error / actual) * 100\n",
    "            print(f\"  {season:6s}: Actual={actual:6.1f}, Predicted={pred:6.1f}, Error={error_pct:4.1f}%\")\n",
    "    \n",
    "    # Create seasonal comparison visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Seasonal Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    seasons = seasonal_comparison.index\n",
    "    x = np.arange(len(seasons))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, seasonal_comparison['Actual'], width, \n",
    "                   label='Actual', color='#1f77b4', alpha=0.8)\n",
    "    bars2 = ax1.bar(x + width/2, seasonal_comparison['Predicted'], width,\n",
    "                   label='Predicted', color='#ff4444', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Season', fontweight='bold')\n",
    "    ax1.set_ylabel('Average Weekly Bikes', fontweight='bold')\n",
    "    ax1.set_title('Seasonal Averages: Actual vs Predicted', fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(seasons)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Error by season\n",
    "    seasonal_errors = [abs(seasonal_comparison.loc[season, 'Actual'] - seasonal_comparison.loc[season, 'Predicted']) \n",
    "                      for season in seasons]\n",
    "    bars3 = ax2.bar(seasons, seasonal_errors, color=['#2ecc71', '#f39c12', '#e74c3c', '#3498db'], alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Season', fontweight='bold')\n",
    "    ax2.set_ylabel('Absolute Prediction Error', fontweight='bold')\n",
    "    ax2.set_title('Prediction Error by Season', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return seasonal_comparison\n",
    "\n",
    "# Analyze seasonal patterns\n",
    "seasonal_analysis = analyze_seasonal_patterns(df_weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "8TBV4heIGz7p",
    "outputId": "27e166d7-77f1-4b7e-9b6d-b5d537374bd0"
   },
   "outputs": [],
   "source": [
    "def create_monthly_bar_comparison(y_actual, y_predicted, start_date='2016-12-01'):\n",
    "    \"\"\"\n",
    "    Create monthly grouped bar comparison\n",
    "    \"\"\"\n",
    "    # Create DataFrame\n",
    "    dates = pd.date_range(start=start_date, periods=len(y_actual), freq='h')\n",
    "    df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Actual': y_actual.ravel() if hasattr(y_actual, 'ravel') else y_actual,\n",
    "        'Predicted': y_predicted.ravel() if hasattr(y_predicted, 'ravel') else y_predicted\n",
    "    })\n",
    "    \n",
    "    # Aggregate to monthly\n",
    "    df_monthly = df.set_index('Date').resample('ME').mean().reset_index()\n",
    "    \n",
    "    print(f\"Monthly data points: {len(df_monthly)}\")\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # Bar positions\n",
    "    x = np.arange(len(df_monthly))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Create bars\n",
    "    bars1 = ax.bar(x - width/2, df_monthly['Actual'], width, \n",
    "                   label='Actual', color='#1f77b4', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    bars2 = ax.bar(x + width/2, df_monthly['Predicted'], width,\n",
    "                   label='Predicted', color='#ff4444', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "               f'{int(height)}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "               f'{int(height)}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Month', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('Average Bikes per Hour', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('Monthly Bike Demand: Actual vs Predicted Comparison', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # X-axis labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([date.strftime('%Y-%m') for date in df_monthly['Date']], \n",
    "                       rotation=45, ha='right')\n",
    "    \n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Monthly statistics\n",
    "    monthly_rmse = np.sqrt(((df_monthly['Actual'] - df_monthly['Predicted'])**2).mean())\n",
    "    print(f\"\\nMonthly Statistics:\")\n",
    "    print(f\"Monthly RMSE: {monthly_rmse:.2f}\")\n",
    "    print(f\"Mean monthly actual: {df_monthly['Actual'].mean():.1f}\")\n",
    "    print(f\"Mean monthly predicted: {df_monthly['Predicted'].mean():.1f}\")\n",
    "    \n",
    "    # Find best and worst months\n",
    "    df_monthly['Error'] = abs(df_monthly['Actual'] - df_monthly['Predicted'])\n",
    "    best_month = df_monthly.loc[df_monthly['Error'].idxmin(), 'Date']\n",
    "    worst_month = df_monthly.loc[df_monthly['Error'].idxmax(), 'Date']\n",
    "    \n",
    "    print(f\"Best prediction month: {best_month.strftime('%Y-%m')} (Error: {df_monthly['Error'].min():.1f})\")\n",
    "    print(f\"Worst prediction month: {worst_month.strftime('%Y-%m')} (Error: {df_monthly['Error'].max():.1f})\")\n",
    "    \n",
    "    return df_monthly\n",
    "\n",
    "# Create monthly bar comparison\n",
    "monthly_bars = create_monthly_bar_comparison(y_actual_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline metrics (from regularized baseline)\n",
    "baseline_train_rmse = math.sqrt(mean_squared_error(y_train, baseline_train_preds))  # Your baseline predictions\n",
    "baseline_test_rmse = math.sqrt(mean_squared_error(y_test, baseline_test_preds))\n",
    "\n",
    "# Final model metrics (from balanced final model)\n",
    "final_train_rmse = math.sqrt(mean_squared_error(y_train, balanced_train_preds))\n",
    "final_test_rmse = math.sqrt(mean_squared_error(y_test, balanced_test_preds))\n",
    "\n",
    "# R² scores\n",
    "baseline_train_r2 = r2_score(y_train, baseline_train_preds)\n",
    "baseline_test_r2 = r2_score(y_test, baseline_test_preds)\n",
    "final_train_r2 = r2_score(y_train, balanced_train_preds)\n",
    "final_test_r2 = r2_score(y_test, balanced_test_preds)\n",
    "\n",
    "# Overfitting gaps\n",
    "baseline_gap = baseline_test_rmse - baseline_train_rmse\n",
    "final_gap = final_test_rmse - final_train_rmse\n",
    "\n",
    "# Create the comparison chart\n",
    "plt.figure(figsize=(16, 6))\n",
    "fig = plt.gcf()\n",
    "fig.patch.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Define colors\n",
    "colors = ['#39A0ED', '#FF5E5B']  # Blue for Baseline, Coral for Final\n",
    "\n",
    "# Model names\n",
    "models = ['Regularized\\nBaseline', 'Final Tuned\\nModel']\n",
    "\n",
    "# RMSE comparison (subplot 1)\n",
    "plt.subplot(1, 3, 1)\n",
    "test_rmse_values = [baseline_test_rmse, final_test_rmse]\n",
    "bars1 = plt.bar(models, test_rmse_values, color=colors,\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.title('Test RMSE Comparison\\n(lower is better)', fontsize=14, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('Test RMSE', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add best performance line\n",
    "min_rmse = min(test_rmse_values)\n",
    "plt.axhline(y=min_rmse, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, min_rmse + min_rmse*0.05, f'Best: {min_rmse:.2f}',\n",
    "         ha='right', va='bottom', color='#333333', alpha=0.7, fontweight='bold')\n",
    "\n",
    "# R² comparison (subplot 2)\n",
    "plt.subplot(1, 3, 2)\n",
    "r2_values = [baseline_test_r2, final_test_r2]\n",
    "bars2 = plt.bar(models, r2_values, color=colors,\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.title('Test R² Comparison\\n(higher is better)', fontsize=14, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('Test R²', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add best performance line\n",
    "max_r2 = max(r2_values)\n",
    "plt.axhline(y=max_r2, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, max_r2 - 0.01, f'Best: {max_r2:.4f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7, fontweight='bold')\n",
    "\n",
    "# Overfitting Gap comparison (subplot 3)\n",
    "plt.subplot(1, 3, 3)\n",
    "gap_values = [baseline_gap, final_gap]\n",
    "bars3 = plt.bar(models, gap_values, color=colors,\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + height*0.02,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.title('Overfitting Gap\\n(lower is better)', fontsize=14, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('Gap (Test - Train RMSE)', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add best performance line\n",
    "min_gap = min(gap_values)\n",
    "plt.axhline(y=min_gap, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, min_gap + min_gap*0.1, f'Best: {min_gap:.2f}',\n",
    "         ha='right', va='bottom', color='#333333', alpha=0.7, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yFkCZCB2WsRp",
    "x6hwTLKoW-d_",
    "QhRPHn3wXK2V"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
