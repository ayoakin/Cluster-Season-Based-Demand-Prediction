{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "71e4c96b77c54d9099a19f9c47ec705d",
    "deepnote_cell_type": "markdown",
    "id": "-M_rHR3pgJgu"
   },
   "source": [
    "# **Experiment Objectives and Hypotheses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5bf23616ba3449acb677f3e7c0a526b3",
    "deepnote_cell_type": "markdown",
    "id": "ic2x8GRggQxg"
   },
   "source": [
    "## **Hypotheses:**\n",
    "\n",
    "\n",
    "\n",
    "*   H1: Training separate regression models for each of the three identified clusters will improve performance (RMSE, MAE, R²) over a single unified model.\n",
    "\n",
    "* H2: Clustering using Mahalanobis distance will yield more effective segments for modeling than Euclidean distance.\n",
    "\n",
    "* H3: Training separate models for each season (Spring, Summer, Fall, Winter) will improve predictive accuracy by capturing seasonal variation in bike rental behavior.\n",
    "\n",
    "* H4: Neural Network outperforms our baseline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Research Questions:**\n",
    "* RQ1: Does cluster-based modeling improve regression performance compared to a single global model?\n",
    "\n",
    "* RQ2: Which clustering distance metric (Mahalanobis vs. Euclidean) produces better downstream model performance?\n",
    "\n",
    "* RQ3: Can season-specific models capture trends in bike demand better than a unified model?\n",
    "\n",
    "* RQ4: Among XGBoost, neural networks, and cluster-wise regressors, which method yields the best performance?\n",
    "\n",
    "\n",
    "\n",
    "## **Experimental Design:**\n",
    "### Baseline Model:\n",
    "XGBoost Regressor\n",
    "### Target Variable:\n",
    "Rented Bike Count\n",
    "### Dataset:\n",
    "Seoul Bike Sharing Demand\n",
    "### Data Split:\n",
    "80% training, 20% testing\n",
    "\n",
    "### Method:\n",
    "[Insert a brief description of your method here]\n",
    "\n",
    "### Visualization:\n",
    "\n",
    "[Insert if applicable]\n",
    "\n",
    "\n",
    "### Performance Metrics:\n",
    "RMSE, MAE, and R²\n",
    "\n",
    "## **Expected Outcomes:**\n",
    "\n",
    "\n",
    "*  [Insert expectations from experiments]\n",
    "*  [Insert expectations from experiments]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "882484e8c27c4781a411807e95784c5e",
    "deepnote_cell_type": "markdown",
    "id": "yFkCZCB2WsRp"
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0730989649d64d0694e9356114f53eee",
    "deepnote_cell_type": "markdown",
    "id": "NalgnChvpx1y"
   },
   "source": [
    "include additional libraries for development in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2128fd93be3c4ab688782a8208ec38e1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "id": "-HsjQHFg_0Yz",
    "outputId": "87ebddb7-7702-496f-f01c-86a9746160f1"
   },
   "outputs": [],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5d4346838c994e8a82f633879327f917",
    "deepnote_cell_type": "code",
    "id": "4nfbZTdB_w0K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "mpl.rcParams['axes.edgecolor'] = '#333333'\n",
    "mpl.rcParams['axes.linewidth'] = 0.8\n",
    "mpl.rcParams['xtick.color'] = '#333333'\n",
    "mpl.rcParams['ytick.color'] = '#333333'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f234dcad2019409c98feb09037de945e",
    "deepnote_cell_type": "markdown",
    "id": "x6hwTLKoW-d_"
   },
   "source": [
    "# **Import Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0f4cb9202c7d4ad3a74fbd8280a154ee",
    "deepnote_cell_type": "code",
    "id": "4sGCzaCo_7EK"
   },
   "outputs": [],
   "source": [
    "# Fetch Seoul Bike Sharing Demand dataset from UCI ML Repository\n",
    "seoul_bike_sharing_demand = fetch_ucirepo(id=560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "68fcab94ff3740018e94a22dd747ffb8",
    "deepnote_cell_type": "code",
    "id": "l2TCiohL_-VU"
   },
   "outputs": [],
   "source": [
    "# Data (as pandas dataframes)\n",
    "X_original = seoul_bike_sharing_demand.data.features\n",
    "y_original = seoul_bike_sharing_demand.data.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "fd1f20be127d44a9882569bec14a366b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "id": "kceoOc1_Cgpp",
    "outputId": "01eb29c1-db0e-471e-c5f0-443132789cbb"
   },
   "outputs": [],
   "source": [
    "# Print dataset information\n",
    "print(\"Dataset Metadata:\")\n",
    "print(seoul_bike_sharing_demand.metadata)\n",
    "print(\"\\nVariable Information:\")\n",
    "print(seoul_bike_sharing_demand.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "54e7eb3496074220a35ce9f45d17010a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "id": "JXhCwGVOBWIE",
    "outputId": "6a140342-dee9-4818-e4b0-edd5f08fbff9"
   },
   "outputs": [],
   "source": [
    "# Examine feature information\n",
    "print(\"\\nOriginal feature columns:\")\n",
    "print(X_original.columns.tolist())\n",
    "print(\"\\nOriginal target variable:\")\n",
    "print(y_original.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f5cc2412934d479c848b2f3a3f200f94",
    "deepnote_cell_type": "markdown",
    "id": "QhRPHn3wXK2V"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3477937f84304b6f84bd6d5e30bd6198",
    "deepnote_cell_type": "markdown",
    "id": "HMA5KVPJqNEH"
   },
   "source": [
    "**Data Preprocessing Steps**\n",
    "\n",
    "\n",
    "\n",
    "*   Makes \"Rented Bike Count\" new target. Move former target from dataset api to features\n",
    "*   Check for missing values\n",
    "*  Transform date column to date time\n",
    "*  Transform categorical features using one hot encoding\n",
    "* Split dataset into test (80%) and training (20%)\n",
    "* Create heatmap to identify correlated features\n",
    "* Compute VIF (Variance Inflation Factor) to measure multicolinearity\n",
    "* Define function to remove colinear features using a 0.8 Colinearity threshold\n",
    "* Compare VIF before and after removal\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6523f13f9e354ca582c6be7855defbd3",
    "deepnote_cell_type": "code",
    "id": "wY0aMkBoBtrX"
   },
   "outputs": [],
   "source": [
    "# Make 'Rented Bike Count' the new target if it exists\n",
    "\n",
    "if 'Rented Bike Count' in X_original.columns:\n",
    "    # Make 'Rented Bike Count' the new target\n",
    "    y = X_original[['Rented Bike Count']]\n",
    "    # Remove 'Rented Bike Count' from features\n",
    "    X = X_original.drop('Rented Bike Count', axis=1)\n",
    "    # Add original target to features\n",
    "    X = pd.concat([X, y_original], axis=1)\n",
    "\n",
    "else:\n",
    "    # If 'Rented Bike Count' is already the target, just confirm\n",
    "    print(\"'Rented Bike Count' is already the target variable.\")\n",
    "    y = y_original\n",
    "    X = X_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1e213cfa00354b2c914335366f0ab8f2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "id": "gpuLenjDAB_4",
    "outputId": "186b7f4f-f22b-4237-9eb0-2f208333c174"
   },
   "outputs": [],
   "source": [
    "print(\"\\nNew feature columns:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"\\nNew target variable:\")\n",
    "print(y.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0e03c8c677e54ec9af3796ddce72a2d2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "id": "ELZ5f9cmAE0W",
    "outputId": "e87a2dd3-abff-4b2f-a74d-7cbbb896900f"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in features:\")\n",
    "print(X.isnull().sum())\n",
    "print(\"\\nMissing values in target:\")\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8e3c601e3444420c9098cfd6fe0ad48f",
    "deepnote_cell_type": "code",
    "id": "T0ZypEjpAjqO"
   },
   "outputs": [],
   "source": [
    "# 1. Convert date column to datetime and extract useful components. date format is DD/MM/YYYY\n",
    "if 'Date' in X.columns:\n",
    "    # Specify the correct date format as DD/MM/YYYY\n",
    "    X['Date'] = pd.to_datetime(X['Date'], format='%d/%m/%Y')\n",
    "    X['Year'] = X['Date'].dt.year\n",
    "    X['Month'] = X['Date'].dt.month\n",
    "    X['Day'] = X['Date'].dt.day\n",
    "    X['DayOfWeek'] = X['Date'].dt.dayofweek\n",
    "    X = X.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "62003c9ef9d3403da0ad84d45be5ed5d",
    "deepnote_cell_type": "code",
    "id": "HqzBrisvEfmV"
   },
   "outputs": [],
   "source": [
    "# 2. Convert categorical features to numeric using one-hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7f52198c0d624e27b9c5856a172c95a9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "deepnote_cell_type": "code",
    "id": "KyW3-YG4QpIi",
    "outputId": "aaf03e76-e9e4-45d4-9a01-890219bc799d"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a7827e45998c4fd2b1c26645266bac90",
    "deepnote_cell_type": "code",
    "id": "pUXcIclqElR5"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3c7cd765e05d4a618a9c930387b08ae6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684
    },
    "deepnote_cell_type": "code",
    "id": "fMNS9RuVTF7w",
    "outputId": "425960a4-cd90-4d89-c2c5-ce819a3b8723"
   },
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "\n",
    "correlation_matrix = X_train_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Use the calculated correlation matrix in the heatmap\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, fmt='.2f')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4a2bbab3dd1847b2b31a13c302295819",
    "deepnote_cell_type": "code",
    "id": "RE8rz2U1Ueay"
   },
   "outputs": [],
   "source": [
    "# Function to calculate VIF (Variance Inflation Factor)\n",
    "def calculate_vif(X):\n",
    "    # Select only numeric columns to avoid issues with non-numeric data types\n",
    "    X_numeric = X.select_dtypes(include=np.number)\n",
    "    # Drop columns with any non-finite values if they exist (though the type error suggests something else)\n",
    "    # X_numeric = X_numeric.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "    # Check if there are still columns left\n",
    "    if X_numeric.shape[1] == 0:\n",
    "        print(\"Warning: No numeric columns found for VIF calculation.\")\n",
    "        return pd.DataFrame(columns=[\"feature\", \"VIF\"])\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X_numeric.columns\n",
    "    # Ensure the values passed to variance_inflation_factor are finite and the correct type\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_numeric.values, i) for i in range(X_numeric.shape[1])]\n",
    "    return vif_data.sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "# Function to remove collinear features based on correlation threshold\n",
    "def remove_collinear_features(X, threshold=0.8):\n",
    "    # Select only numeric columns before calculating correlation\n",
    "    X_numeric = X.select_dtypes(include=np.number)\n",
    "\n",
    "    corr_matrix = X_numeric.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Plot correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=False,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    print(f\"Dropping {len(to_drop)} collinear features: {', '.join(to_drop)}\")\n",
    "\n",
    "    # Return dataframe with collinear features removed (applied to original X)\n",
    "    return X.drop(to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a20e915d576b4541ab4f21cc779720f6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deepnote_cell_type": "code",
    "id": "ROXG-v8eVN_k",
    "outputId": "0465025f-5402-40f1-bf34-272e6fa6f1ba"
   },
   "outputs": [],
   "source": [
    "# Calculate VIF before feature removal\n",
    "print(\"Variance Inflation Factors before colinearity removal:\")\n",
    "# Ensure only numeric columns are passed to calculate_vif\n",
    "X_train_numeric = X_train.select_dtypes(include=np.number)\n",
    "vif_before = calculate_vif(X_train_numeric)\n",
    "print(vif_before.head(10))  # Show top 10 highest VIF values\n",
    "\n",
    "# Remove collinear features\n",
    "# The remove_collinear_features function now handles numeric selection internally for correlation\n",
    "X_train_filtered = remove_collinear_features(X_train, threshold=0.8)\n",
    "# Ensure test set has the same columns as the filtered training set\n",
    "X_test_filtered = X_test[X_train_filtered.columns]\n",
    "\n",
    "# Calculate VIF after feature removal\n",
    "print(\"\\nVariance Inflation Factors after collinearity removal:\")\n",
    "# Ensure only numeric columns are passed to calculate_vif\n",
    "X_train_filtered_numeric = X_train_filtered.select_dtypes(include=np.number)\n",
    "vif_after = calculate_vif(X_train_filtered_numeric)\n",
    "print(vif_after.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d6c17a67d65d4583ad1ce0e9bb4a268c",
    "deepnote_cell_type": "code",
    "id": "P8x5rCrwUSxP"
   },
   "outputs": [],
   "source": [
    "# Standardize the filtered features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
    "X_test_scaled = scaler.transform(X_test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f4935c1799804b0b9d3c977869fef918",
    "deepnote_cell_type": "markdown",
    "id": "qthduIVYs-Lp"
   },
   "source": [
    "# **Baseline Model Development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "21875369fab84c83a60896e63a8b94f6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "id": "aT6nh24-tDCV",
    "outputId": "7d0a4eb0-22ad-479a-8344-6076afd16535"
   },
   "outputs": [],
   "source": [
    "# Baseline XGBoost model\n",
    "print(\"Training baseline XGBoost regression model...\")\n",
    "baseline_model = xgb.XGBRegressor(random_state=42)\n",
    "baseline_model.fit(X_train_scaled, y_train.values.ravel())\n",
    "baseline_preds = baseline_model.predict(X_test_scaled)\n",
    "baseline_metrics = {\n",
    "    'rmse': math.sqrt(mean_squared_error(y_test, baseline_preds)),\n",
    "    'r2': r2_score(y_test, baseline_preds)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f03bca1526e64ec29f9302f00b2a2662",
    "deepnote_cell_type": "markdown",
    "id": "Bfs7TqaNXbmM"
   },
   "source": [
    "# **Model Development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9a6110a6bb324fa0b2fd1aa3a6b59e4a",
    "deepnote_cell_type": "code",
    "id": "YKoSCnIHtypv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "286e847c30d44ba2b8f4362f7fb4abc8",
    "deepnote_cell_type": "markdown",
    "id": "IjdW7iiSgAOi"
   },
   "source": [
    "# **Evaluation and Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2665100ac69e4827a00abd322c2c2330",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "deepnote_cell_type": "code",
    "id": "BJmBYBA0HOpj",
    "outputId": "75b61a9e-ab7f-4b40-c2b2-36bf7c4fd97a"
   },
   "outputs": [],
   "source": [
    "# Create a bar chart comparing the metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "fig.patch.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Define the colors from our palette\n",
    "colors = ['#39A0ED', '#FF5E5B', '#D8D8D8']  # Blue, Coral, Light Gray\n",
    "\n",
    "# RMSE comparison (lower is better)\n",
    "plt.subplot(1, 2, 1)\n",
    "models = ['Baseline', 'Model_developed_1', 'Model_developed_2']\n",
    "rmse_values = [baseline_metrics['rmse'], Model_developed_1['rmse'], Model_developed_2['rmse']]\n",
    "bars = plt.bar(models, rmse_values, color=colors,\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.title('RMSE Comparison (lower is better)', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('RMSE', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "min_rmse = min(rmse_values)\n",
    "plt.axhline(y=min_rmse, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, min_rmse-0.5, f'Best: {min_rmse:.2f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d7ccdd72203d467580a774da2a176429",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "deepnote_cell_type": "code",
    "id": "5XRc09OPHdH4",
    "outputId": "46f9c8b5-10f6-4920-e66d-c9002971dd7a"
   },
   "outputs": [],
   "source": [
    "# R² comparison (higher is better)\n",
    "plt.subplot(1, 2, 2)\n",
    "r2_values = [baseline_metrics['r2'], Model_developed_1['r2'], Model_developed_2['r2']]\n",
    "bars = plt.bar(models, r2_values, color=colors,  # Using the same color palette\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Customize appearance\n",
    "plt.title('R² Comparison (higher is better)', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('R²', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add a horizontal line at the maximum R² for reference\n",
    "max_r2 = max(r2_values)\n",
    "plt.axhline(y=max_r2, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, max_r2-0.02, f'Best: {max_r2:.4f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('seoul_bike_metrics_comparison.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "92f55d6f98ad42e591e7c91a9d66976a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "deepnote_cell_type": "code",
    "id": "zWQIwtG0dJSE",
    "outputId": "bfcb9b43-9a66-4338-e3d3-98ad6fe1ec5d"
   },
   "outputs": [],
   "source": [
    "# MAE comparison (lower is better)\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Calculate MAE for each model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "Model_developed_1_mae = mean_absolute_error(y_test, Model_developed_1_preds)\n",
    "Model_developed_2_mae = mean_absolute_error(y_test, Model_developed_2_preds)\n",
    "\n",
    "mae_values = [baseline_mae, Model_developed_1, Model_developed_2]\n",
    "bars = plt.bar(models, mae_values, color=colors,  # Using the same color palette\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.title('MAE Comparison (lower is better)', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('MAE', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add a horizontal line at the minimum MAE for reference\n",
    "min_mae = min(mae_values)\n",
    "plt.axhline(y=min_mae, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, min_mae-0.5, f'Best: {min_mae:.2f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('seoul_bike_metrics_comparison.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3609b8234d304dea9d608f935bd3974c",
    "deepnote_cell_type": "code",
    "execution_context_id": "1d1c2c67-0cab-4919-87f0-bf80c4b04155",
    "execution_millis": 8,
    "execution_start": 1748099563540,
    "id": "tB0MCZ695sf-",
    "source_hash": "9bcc7e12"
   },
   "outputs": [],
   "source": [
    "# Update imports to use correct location for ColumnTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.compose import ColumnTransformer  # Changed from sklearn.preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1e9829a2f414421588453f1a8c976c54",
    "deepnote_cell_type": "code",
    "execution_context_id": "1d1c2c67-0cab-4919-87f0-bf80c4b04155",
    "execution_millis": 268,
    "execution_start": 1748099899233,
    "id": "oe6vtIHO5sf-",
    "outputId": "9abd4f47-7548-4b9d-fd7b-06425f4613ee",
    "source_hash": "ab059a44"
   },
   "outputs": [],
   "source": [
    "# First import pandas and numpy and other required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Seoul Bike Sharing dataset\n",
    "df = pd.read_csv('SeoulBikeData.csv')\n",
    "\n",
    "# The error occurs because the column name in the CSV might be different\n",
    "# Let's first check the actual column names in the dataset\n",
    "print(\"Available columns in dataset:\", df.columns.tolist())\n",
    "\n",
    "# Get original features and target\n",
    "X_original = df.drop('Rented Bike Count', axis=1)\n",
    "y_original = df[['Rented Bike Count']]\n",
    "\n",
    "# Load and prepare the data\n",
    "y = df[['Rented Bike Count']]\n",
    "X = df.drop('Rented Bike Count', axis=1)\n",
    "\n",
    "# Convert date column to datetime and extract useful components\n",
    "if 'Date' in X.columns:\n",
    "    X['Date'] = pd.to_datetime(X['Date'], format='%d/%m/%Y')\n",
    "    X['Year'] = X['Date'].dt.year\n",
    "    X['Month'] = X['Date'].dt.month\n",
    "    X['Day'] = X['Date'].dt.day\n",
    "    X['DayOfWeek'] = X['Date'].dt.dayofweek\n",
    "    X = X.drop('Date', axis=1)\n",
    "\n",
    "# Convert categorical features to numeric using one-hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Create train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the filtered dataframes with scaled values\n",
    "X_train_filtered = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_filtered = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Print available columns in the filtered dataset to debug\n",
    "print(\"\\nAvailable columns in X_train_filtered:\", X_train_filtered.columns.tolist())\n",
    "\n",
    "# Create cyclical features for hour\n",
    "if 'Hour' in X_train_filtered.columns:\n",
    "    X_train_filtered['Hour_sin'] = np.sin(2 * np.pi * X_train_filtered['Hour'] / 24)\n",
    "    X_train_filtered['Hour_cos'] = np.cos(2 * np.pi * X_train_filtered['Hour'] / 24)\n",
    "    X_test_filtered['Hour_sin'] = np.sin(2 * np.pi * X_test_filtered['Hour'] / 24)\n",
    "    X_test_filtered['Hour_cos'] = np.cos(2 * np.pi * X_test_filtered['Hour'] / 24)\n",
    "\n",
    "# Create interaction features - using exact column names from the dataset\n",
    "# The column might be named differently in the CSV, possibly 'Temp' instead of 'Temperature'\n",
    "# We'll add a check to use the correct column names\n",
    "temp_col = 'Temperature' if 'Temperature' in X_train_filtered.columns else 'Temp'\n",
    "humidity_col = 'Humidity' if 'Humidity' in X_train_filtered.columns else 'Humidity (%)'\n",
    "\n",
    "if temp_col in X_train_filtered.columns and 'Hour' in X_train_filtered.columns:\n",
    "    X_train_filtered['Temp_Hour'] = X_train_filtered[temp_col] * X_train_filtered['Hour']\n",
    "    X_test_filtered['Temp_Hour'] = X_test_filtered[temp_col] * X_test_filtered['Hour']\n",
    "\n",
    "if temp_col in X_train_filtered.columns and humidity_col in X_train_filtered.columns:\n",
    "    X_train_filtered['Humidity_Temp'] = X_train_filtered[humidity_col] * X_train_filtered[temp_col]\n",
    "    X_test_filtered['Humidity_Temp'] = X_test_filtered[humidity_col] * X_test_filtered[temp_col]\n",
    "\n",
    "print(\"\\nEnhanced features created\")\n",
    "print(\"New feature columns:\", [col for col in X_train_filtered.columns if col not in X_train.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6d89d9d8382b4c1cbcf51f384f0ce084",
    "deepnote_cell_type": "code",
    "execution_context_id": "1d1c2c67-0cab-4919-87f0-bf80c4b04155",
    "execution_millis": 1,
    "execution_start": 1748099965421,
    "id": "mo3cfNan5sf-",
    "outputId": "b479f370-478e-446e-fa1a-f83d25c1e5a6",
    "source_hash": "c0bbf0fb"
   },
   "outputs": [],
   "source": [
    "# Prepare features for neural network\n",
    "# Looking at the actual column names from earlier output, let's fix the feature names\n",
    "numeric_features = ['Hour', 'Temperature(�C)', 'Humidity(%)', 'Wind speed (m/s)',\n",
    "                   'Visibility (10m)', 'Dew point temperature(�C)', 'Solar Radiation (MJ/m2)',\n",
    "                   'Rainfall(mm)', 'Snowfall (cm)', 'Year', 'Month', 'Day', 'DayOfWeek']\n",
    "\n",
    "categorical_features = ['Seasons_Spring', 'Seasons_Summer', 'Seasons_Winter',\n",
    "                       'Holiday_No Holiday', 'Functioning Day_Yes']\n",
    "\n",
    "# First create the additional features\n",
    "# Add cyclical hour features\n",
    "X['Hour_sin'] = np.sin(2 * np.pi * X['Hour'] / 24)\n",
    "X['Hour_cos'] = np.cos(2 * np.pi * X['Hour'] / 24)\n",
    "\n",
    "# Add interaction features - using the correct column names from the dataset\n",
    "X['Temp_Hour'] = X['Temperature(�C)'] * X['Hour']\n",
    "X['Humidity_Temp'] = X['Humidity(%)'] * X['Temperature(�C)']\n",
    "\n",
    "# Update numeric features to include new derived features\n",
    "numeric_features.extend(['Hour_sin', 'Hour_cos', 'Temp_Hour', 'Humidity_Temp'])\n",
    "\n",
    "# Create feature matrix for neural network\n",
    "X_nn = pd.concat([X[numeric_features], X[categorical_features]], axis=1)\n",
    "\n",
    "print(\"Neural network feature matrix shape:\", X_nn.shape)\n",
    "print(\"Features:\", X_nn.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "578f82ef97b846b984e3339845f77e58",
    "deepnote_cell_type": "code",
    "execution_context_id": "1d1c2c67-0cab-4919-87f0-bf80c4b04155",
    "execution_millis": 14,
    "execution_start": 1748099986480,
    "id": "rcULKMMC5sf-",
    "outputId": "6c1d9720-d9d2-45ff-cef4-3efbe998d892",
    "source_hash": "e71bcc22"
   },
   "outputs": [],
   "source": [
    "# Split data for neural network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data\n",
    "X_train_val, X_test_nn, y_train_val, y_test_nn = train_test_split(\n",
    "    X_nn, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_nn)\n",
    "X_val_scaled = scaler.transform(X_val_nn)\n",
    "X_test_scaled = scaler.transform(X_test_nn)\n",
    "\n",
    "print(\"Neural Network Data Shapes:\")\n",
    "print(f\"X_train: {X_train_scaled.shape}\")\n",
    "print(f\"X_val: {X_val_scaled.shape}\")\n",
    "print(f\"X_test: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "01f5c1e561aa45498c1a7406658ad744",
    "deepnote_cell_type": "code",
    "execution_context_id": "1d1c2c67-0cab-4919-87f0-bf80c4b04155",
    "execution_millis": 262,
    "execution_start": 1748100003220,
    "id": "pSiw7aXE5sf-",
    "outputId": "c3af6016-e1bd-4ed1-a764-f52a3d9192c0",
    "source_hash": "4cef324f"
   },
   "outputs": [],
   "source": [
    "# Build and compile the neural network model\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"Neural network model built and compiled successfully\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "207f24f1b8c4440ea8f0c0da5187deb2",
    "deepnote_cell_type": "code",
    "execution_context_id": "1d1c2c67-0cab-4919-87f0-bf80c4b04155",
    "execution_millis": 50559,
    "execution_start": 1748100038311,
    "id": "ubtFNTNZ5sf-",
    "outputId": "7bd22167-42ff-4730-aa34-13f6e1e0a91e",
    "source_hash": "5afdee51"
   },
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    'best_seoul_bike_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training neural network...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_nn,\n",
    "    validation_data=(X_val_scaled, y_val_nn),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Neural network training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b84c87b20acf4f03b429686931cd8975",
    "deepnote_cell_type": "code",
    "execution_context_id": "1d1c2c67-0cab-4919-87f0-bf80c4b04155",
    "execution_millis": 131,
    "execution_start": 1748100116781,
    "id": "6ssNYXR_5sf-",
    "outputId": "8e67d6d1-0795-4147-afd7-6f05f837e1d9",
    "source_hash": "299c2f9d"
   },
   "outputs": [],
   "source": [
    "# Evaluate neural network performance\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test_nn, y_pred_nn))\n",
    "mae_nn = mean_absolute_error(y_test_nn, y_pred_nn)\n",
    "r2_nn = r2_score(y_test_nn, y_pred_nn)\n",
    "\n",
    "print(\"Neural Network Performance Metrics:\")\n",
    "print(f\"RMSE: {rmse_nn:.2f}\")\n",
    "print(f\"MAE: {mae_nn:.2f}\")\n",
    "print(f\"R²: {r2_nn:.4f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "nn_results = {\n",
    "    'Model': 'Neural Network',\n",
    "    'RMSE': rmse_nn,\n",
    "    'MAE': mae_nn,\n",
    "    'R²': r2_nn\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "fa4f2e3550654afcb708d04ef3ab6cdd",
    "deepnote_cell_type": "code",
    "execution_context_id": "1d1c2c67-0cab-4919-87f0-bf80c4b04155",
    "execution_millis": 985,
    "execution_start": 1748100157430,
    "id": "cWdGWGE25sf_",
    "outputId": "ac73269f-dc28-47c4-d161-0c2bd5f07623",
    "source_hash": "4343c238"
   },
   "outputs": [],
   "source": [
    "# Import matplotlib first\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize neural network results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training history\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Neural Network Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Neural Network Training MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "# Predictions vs Actual\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(y_test_nn, y_pred_nn, alpha=0.5)\n",
    "plt.plot([y_test_nn.min(), y_test_nn.max()], [y_test_nn.min(), y_test_nn.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Neural Network: R² = {r2_nn:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "936d0c2acbae4addbb1e508ca7a82f1c",
    "deepnote_cell_type": "code",
    "execution_context_id": "1d1c2c67-0cab-4919-87f0-bf80c4b04155",
    "execution_millis": 1,
    "execution_start": 1748100180270,
    "id": "7HEisWkQ5sf_",
    "outputId": "e27e555f-7584-4d7c-b329-b90217aa5e16",
    "source_hash": "61ad904e"
   },
   "outputs": [],
   "source": [
    "# Compare with baseline (add this after you implement your XGBoost baseline)\n",
    "# This code assumes you have baseline results stored\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display neural network results\n",
    "print(f\"Neural Network - RMSE: {rmse_nn:.2f}, MAE: {mae_nn:.2f}, R²: {r2_nn:.4f}\")\n",
    "\n",
    "# Add comparison with other models when available\n",
    "print(\"\\nNeural Network successfully integrated into the Seoul Bike prediction framework!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "deepnote_notebook_id": "80a98a6b1f66468f8f2c99510e22f7c5",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
