{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-M_rHR3pgJgu"
   },
   "source": [
    "# **Objectives and Hypotheses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic2x8GRggQxg"
   },
   "source": [
    "## **Primary Hypotheses:**\n",
    "### Cluster-Specific Modeling Hypothesis:\n",
    "Training separate XGBoost regression models for distinct unsupervised data clusters will yield improved predictive performance (lower RMSE and MAE, higher R²) compared to a single model trained on the entire dataset.\n",
    "\n",
    "### Distance Metric Hypothesis:\n",
    "K-means clustering using Mahalanobis distance will produce more meaningful and effective clusters for bike rental prediction than traditional Euclidean distance, resulting in better model performance metrics.\n",
    "\n",
    "## **Experimental Design:**\n",
    "### Baseline Model:\n",
    "XGBoost Regressor\n",
    "### Target Variable:\n",
    "Rented Bike Count\n",
    "### Dataset:\n",
    "Seoul Bike Sharing Demand\n",
    "### Data Split:\n",
    "80% training, 20% testing\n",
    "### Clustering:\n",
    "K-means with k=3, using both Euclidean and Mahalanobis distances\n",
    "\n",
    "### Visualization:\n",
    "PCA for dimensionality reduction and cluster visualization\n",
    "\n",
    "### Performance Metrics:\n",
    "RMSE, MAE, and R²\n",
    "\n",
    "## **Expected Outcomes:**\n",
    "\n",
    "\n",
    "*  Cluster-specific models will capture local patterns within each cluster that might be overlooked by a global model, leading to improved overall prediction accuracy.\n",
    "*  Mahalanobis distance-based clustering will better account for feature correlations and variance differences in the Seoul Bike dataset, resulting in more meaningful clusters and subsequently better predictive performance than Euclidean-based clustering.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFkCZCB2WsRp"
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HsjQHFg_0Yz",
    "outputId": "0381b12f-74d3-4c89-cfd0-1316d5e81cc3"
   },
   "outputs": [],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nfbZTdB_w0K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "mpl.rcParams['axes.edgecolor'] = '#333333'\n",
    "mpl.rcParams['axes.linewidth'] = 0.8\n",
    "mpl.rcParams['xtick.color'] = '#333333'\n",
    "mpl.rcParams['ytick.color'] = '#333333'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6hwTLKoW-d_"
   },
   "source": [
    "# **Import Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sGCzaCo_7EK"
   },
   "outputs": [],
   "source": [
    "# Fetch Seoul Bike Sharing Demand dataset from UCI ML Repository\n",
    "seoul_bike_sharing_demand = fetch_ucirepo(id=560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2TCiohL_-VU"
   },
   "outputs": [],
   "source": [
    "# Data (as pandas dataframes)\n",
    "X_original = seoul_bike_sharing_demand.data.features\n",
    "y_original = seoul_bike_sharing_demand.data.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kceoOc1_Cgpp",
    "outputId": "0cab3c4a-da7a-466b-bbbe-6cc34c0e864d"
   },
   "outputs": [],
   "source": [
    "# Print dataset information\n",
    "print(\"Dataset Metadata:\")\n",
    "print(seoul_bike_sharing_demand.metadata)\n",
    "print(\"\\nVariable Information:\")\n",
    "print(seoul_bike_sharing_demand.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXhCwGVOBWIE",
    "outputId": "54c641ae-6576-42b2-cd35-cb4c56a07089"
   },
   "outputs": [],
   "source": [
    "# Examine feature information\n",
    "print(\"\\nOriginal feature columns:\")\n",
    "print(X_original.columns.tolist())\n",
    "print(\"\\nOriginal target variable:\")\n",
    "print(y_original.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhRPHn3wXK2V"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wY0aMkBoBtrX"
   },
   "outputs": [],
   "source": [
    "# Make 'Rented Bike Count' the new target if it exists\n",
    "\n",
    "if 'Rented Bike Count' in X_original.columns:\n",
    "    # Make 'Rented Bike Count' the new target\n",
    "    y = X_original[['Rented Bike Count']]\n",
    "    # Remove 'Rented Bike Count' from features\n",
    "    X = X_original.drop('Rented Bike Count', axis=1)\n",
    "    # Add original target to features\n",
    "    X = pd.concat([X, y_original], axis=1)\n",
    "\n",
    "else:\n",
    "    # If 'Rented Bike Count' is already the target, just confirm\n",
    "    print(\"'Rented Bike Count' is already the target variable.\")\n",
    "    y = y_original\n",
    "    X = X_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpuLenjDAB_4",
    "outputId": "5aa10a1a-b8a6-436a-bd99-73bfe3ed4407"
   },
   "outputs": [],
   "source": [
    "print(\"\\nNew feature columns:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"\\nNew target variable:\")\n",
    "print(y.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELZ5f9cmAE0W",
    "outputId": "95d2874b-8e8d-4751-d7c2-57825f5d8884"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in features:\")\n",
    "print(X.isnull().sum())\n",
    "print(\"\\nMissing values in target:\")\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0ZypEjpAjqO"
   },
   "outputs": [],
   "source": [
    "# 1. Convert date column to datetime and extract useful components. date format is DD/MM/YYYY\n",
    "if 'Date' in X.columns:\n",
    "    # Specify the correct date format as DD/MM/YYYY\n",
    "    X['Date'] = pd.to_datetime(X['Date'], format='%d/%m/%Y')\n",
    "    X['Year'] = X['Date'].dt.year\n",
    "    X['Month'] = X['Date'].dt.month\n",
    "    X['Day'] = X['Date'].dt.day\n",
    "    X['DayOfWeek'] = X['Date'].dt.dayofweek\n",
    "    X = X.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqzBrisvEfmV"
   },
   "outputs": [],
   "source": [
    "# 2. Convert categorical features to numeric using one-hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "KyW3-YG4QpIi",
    "outputId": "f77d5bc5-2961-4612-b0ee-d04a297595d4"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUXcIclqElR5"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "id": "fMNS9RuVTF7w",
    "outputId": "1f6e6fda-0d94-4e95-a53e-976f0aa0e5ba"
   },
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "\n",
    "correlation_matrix = X_train_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Use the calculated correlation matrix in the heatmap\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, fmt='.2f')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE8rz2U1Ueay"
   },
   "outputs": [],
   "source": [
    "# Function to calculate VIF (Variance Inflation Factor)\n",
    "def calculate_vif(X):\n",
    "    # Select only numeric columns to avoid issues with non-numeric data types\n",
    "    X_numeric = X.select_dtypes(include=np.number)\n",
    "    # Drop columns with any non-finite values if they exist (though the type error suggests something else)\n",
    "    # X_numeric = X_numeric.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "    # Check if there are still columns left\n",
    "    if X_numeric.shape[1] == 0:\n",
    "        print(\"Warning: No numeric columns found for VIF calculation.\")\n",
    "        return pd.DataFrame(columns=[\"feature\", \"VIF\"])\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X_numeric.columns\n",
    "    # Ensure the values passed to variance_inflation_factor are finite and the correct type\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_numeric.values, i) for i in range(X_numeric.shape[1])]\n",
    "    return vif_data.sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "# Function to remove collinear features based on correlation threshold\n",
    "def remove_collinear_features(X, threshold=0.8):\n",
    "    # Select only numeric columns before calculating correlation\n",
    "    X_numeric = X.select_dtypes(include=np.number)\n",
    "\n",
    "    corr_matrix = X_numeric.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Plot correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=False,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    print(f\"Dropping {len(to_drop)} collinear features: {', '.join(to_drop)}\")\n",
    "\n",
    "    # Return dataframe with collinear features removed (applied to original X)\n",
    "    return X.drop(to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ROXG-v8eVN_k",
    "outputId": "3ceae501-a56e-4ecb-f30e-3d5bb624ae5d"
   },
   "outputs": [],
   "source": [
    "# Calculate VIF before feature removal\n",
    "print(\"Variance Inflation Factors before colinearity removal:\")\n",
    "# Ensure only numeric columns are passed to calculate_vif\n",
    "X_train_numeric = X_train.select_dtypes(include=np.number)\n",
    "vif_before = calculate_vif(X_train_numeric)\n",
    "print(vif_before.head(10))  # Show top 10 highest VIF values\n",
    "\n",
    "# Remove collinear features\n",
    "# The remove_collinear_features function now handles numeric selection internally for correlation\n",
    "X_train_filtered = remove_collinear_features(X_train, threshold=0.8)\n",
    "# Ensure test set has the same columns as the filtered training set\n",
    "X_test_filtered = X_test[X_train_filtered.columns]\n",
    "\n",
    "# Calculate VIF after feature removal\n",
    "print(\"\\nVariance Inflation Factors after collinearity removal:\")\n",
    "# Ensure only numeric columns are passed to calculate_vif\n",
    "X_train_filtered_numeric = X_train_filtered.select_dtypes(include=np.number)\n",
    "vif_after = calculate_vif(X_train_filtered_numeric)\n",
    "print(vif_after.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8x5rCrwUSxP"
   },
   "outputs": [],
   "source": [
    "# Standardize the filtered features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
    "X_test_scaled = scaler.transform(X_test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfs7TqaNXbmM"
   },
   "source": [
    "# **Model Development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FeIVFajEte5",
    "outputId": "0eaa5832-c400-461c-d818-e0c6af4b4466"
   },
   "outputs": [],
   "source": [
    "# Baseline XGBoost model\n",
    "print(\"Training baseline XGBoost regression model...\")\n",
    "baseline_model = xgb.XGBRegressor(random_state=42)\n",
    "baseline_model.fit(X_train_scaled, y_train.values.ravel())\n",
    "baseline_preds = baseline_model.predict(X_test_scaled)\n",
    "baseline_metrics = {\n",
    "    'rmse': math.sqrt(mean_squared_error(y_test, baseline_preds)),\n",
    "    'r2': r2_score(y_test, baseline_preds)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0Q5FRVRFV4R",
    "outputId": "95a91c7f-0746-40e5-828c-a97b11f859e8"
   },
   "outputs": [],
   "source": [
    "# Euclidean K-means clustering\n",
    "print(\"Performing Euclidean K-means clustering...\")\n",
    "euclidean_kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "euclidean_clusters = euclidean_kmeans.fit_predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URzgLOpiFfG1",
    "outputId": "1ba4ae2a-ae50-431b-c5ca-27cf9ca8c005"
   },
   "outputs": [],
   "source": [
    "# Mahalanobis K-means clustering\n",
    "print(\"Performing Mahalanobis distance-based clustering...\")\n",
    "# Calculate covariance matrix\n",
    "cov = np.cov(X_train_scaled, rowvar=False)\n",
    "# Add small value to diagonal to ensure matrix is invertible\n",
    "cov += np.eye(cov.shape[0]) * 1e-6\n",
    "inv_cov = np.linalg.inv(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMQv2vD5FpcA"
   },
   "outputs": [],
   "source": [
    "# Function to calculate Mahalanobis distance\n",
    "def mahalanobis_distance(x, y, inv_cov):\n",
    "    return mahalanobis(x, y, inv_cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyZwXnmrF6w1"
   },
   "outputs": [],
   "source": [
    "# Custom K-means with Mahalanobis distance\n",
    "# Initialize with Euclidean K-means\n",
    "mahalanobis_clusters = euclidean_kmeans.predict(X_train_scaled)\n",
    "\n",
    "# Refine clusters using Mahalanobis distance\n",
    "centroids = np.array([X_train_scaled[mahalanobis_clusters == i].mean(axis=0) for i in range(3)])\n",
    "for _ in range(5):  # Limited iterations for simplicity\n",
    "    # Assign points to nearest centroid using Mahalanobis distance\n",
    "    mahalanobis_clusters = np.zeros(len(X_train_scaled), dtype=int)\n",
    "    for i, point in enumerate(X_train_scaled):\n",
    "        distances = [mahalanobis_distance(point, centroid, inv_cov) for centroid in centroids]\n",
    "        mahalanobis_clusters[i] = np.argmin(distances)\n",
    "\n",
    "    # Update centroids\n",
    "    for i in range(3):\n",
    "        if sum(mahalanobis_clusters == i) > 0:\n",
    "            centroids[i] = X_train_scaled[mahalanobis_clusters == i].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBaSZWqaF8ku",
    "outputId": "92ced497-202f-4e1b-f6a6-8d4d21d200a2"
   },
   "outputs": [],
   "source": [
    "# Train cluster-specific models\n",
    "print(\"Training cluster-specific XGBoost regression models...\")\n",
    "euclidean_models = []\n",
    "mahalanobis_models = []\n",
    "\n",
    "for i in range(3):\n",
    "    # Euclidean cluster model\n",
    "    cluster_indices = euclidean_clusters == i\n",
    "    if sum(cluster_indices) > 0:\n",
    "        print(f\"Training Euclidean cluster {i} model with {sum(cluster_indices)} samples\")\n",
    "        model = xgb.XGBRegressor(random_state=42)\n",
    "        model.fit(X_train_scaled[cluster_indices], y_train.iloc[cluster_indices].values.ravel())\n",
    "        euclidean_models.append(model)\n",
    "    else:\n",
    "        euclidean_models.append(None)\n",
    "\n",
    "    # Mahalanobis cluster model\n",
    "    cluster_indices = mahalanobis_clusters == i\n",
    "    if sum(cluster_indices) > 0:\n",
    "        print(f\"Training Mahalanobis cluster {i} model with {sum(cluster_indices)} samples\")\n",
    "        model = xgb.XGBRegressor(random_state=42)\n",
    "        model.fit(X_train_scaled[cluster_indices], y_train.iloc[cluster_indices].values.ravel())\n",
    "        mahalanobis_models.append(model)\n",
    "    else:\n",
    "        mahalanobis_models.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLGUIVDLGPL3"
   },
   "outputs": [],
   "source": [
    "# Predict test set clusters\n",
    "euclidean_test_clusters = euclidean_kmeans.predict(X_test_scaled)\n",
    "mahalanobis_test_clusters = np.zeros(len(X_test_scaled), dtype=int)\n",
    "for i, point in enumerate(X_test_scaled):\n",
    "    distances = [mahalanobis_distance(point, centroid, inv_cov) for centroid in centroids]\n",
    "    mahalanobis_test_clusters[i] = np.argmin(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9U_8PadnGTZa",
    "outputId": "ddbc8931-fb27-46da-d623-08330f9d847c"
   },
   "outputs": [],
   "source": [
    "# Make predictions using cluster-specific models\n",
    "print(\"Making predictions with cluster-specific models...\")\n",
    "euclidean_preds = np.zeros(len(y_test))\n",
    "mahalanobis_preds = np.zeros(len(y_test))\n",
    "\n",
    "for i in range(3):\n",
    "    cluster_indices = euclidean_test_clusters == i\n",
    "    if sum(cluster_indices) > 0 and euclidean_models[i] is not None:\n",
    "        euclidean_preds[cluster_indices] = euclidean_models[i].predict(X_test_scaled[cluster_indices])\n",
    "\n",
    "    cluster_indices = mahalanobis_test_clusters == i\n",
    "    if sum(cluster_indices) > 0 and mahalanobis_models[i] is not None:\n",
    "        mahalanobis_preds[cluster_indices] = mahalanobis_models[i].predict(X_test_scaled[cluster_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bpufze6Ggd2"
   },
   "outputs": [],
   "source": [
    "# Calculate regression metrics\n",
    "euclidean_metrics = {\n",
    "    'rmse': math.sqrt(mean_squared_error(y_test, euclidean_preds)),\n",
    "    'r2': r2_score(y_test, euclidean_preds)\n",
    "}\n",
    "\n",
    "mahalanobis_metrics = {\n",
    "    'rmse': math.sqrt(mean_squared_error(y_test, mahalanobis_preds)),\n",
    "    'r2': r2_score(y_test, mahalanobis_preds)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjdW7iiSgAOi"
   },
   "source": [
    "# **Evaluation and Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LaPtYSDfZDMt",
    "outputId": "839b0fe2-fa67-4372-aca8-0eb21e090742"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "euclidean_palette = ['#FF5E5B', '#D8D8D8', '#39A0ED']  # Coral, Light Gray, Blue\n",
    "mahalanobis_palette = ['#FFD166', '#06D6A0', '#118AB2']  # Yellow, Teal, Blue\n",
    "\n",
    "\n",
    "euclidean_palette = ['#FF5E5B', '#D8D8D8', '#39A0ED']  # Coral, Light Gray, Blue\n",
    "mahalanobis_palette = ['#FFD166', '#06D6A0', '#118AB2']  # Yellow, Teal, Blue\n",
    "\n",
    "\n",
    "print(\"Generating enhanced 3D PCA visualization...\")\n",
    "pca = PCA(n_components=3)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18, 9))\n",
    "fig.patch.set_facecolor('#f8f9fa')  # Light background for the entire figure\n",
    "\n",
    "# Euclidean clusters - 3D plot\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "for i in range(3):\n",
    "    cluster_points = X_train_pca[euclidean_clusters == i]\n",
    "    ax1.scatter(\n",
    "        cluster_points[:, 0],\n",
    "        cluster_points[:, 1],\n",
    "        cluster_points[:, 2],\n",
    "        s=50,  # Marker size\n",
    "        c=[euclidean_palette[i]],\n",
    "        label=f'Cluster {i}',\n",
    "        alpha=0.8,\n",
    "        edgecolors='w',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "# Euclidean plot\n",
    "ax1.set_title('Euclidean K-means Clusters', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "ax1.set_xlabel(f'PC1 ({explained_variance[0]:.1f}%)', fontsize=12, labelpad=10)\n",
    "ax1.set_ylabel(f'PC2 ({explained_variance[1]:.1f}%)', fontsize=12, labelpad=10)\n",
    "ax1.set_zlabel(f'PC3 ({explained_variance[2]:.1f}%)', fontsize=12, labelpad=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.xaxis.pane.fill = False\n",
    "ax1.yaxis.pane.fill = False\n",
    "ax1.zaxis.pane.fill = False\n",
    "ax1.xaxis.pane.set_edgecolor('w')\n",
    "ax1.yaxis.pane.set_edgecolor('w')\n",
    "ax1.zaxis.pane.set_edgecolor('w')\n",
    "ax1.view_init(elev=30, azim=45)  # Set viewing angle\n",
    "\n",
    "# Mahalanobis clusters - 3D plot\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "for i in range(3):\n",
    "    cluster_points = X_train_pca[mahalanobis_clusters == i]\n",
    "    ax2.scatter(\n",
    "        cluster_points[:, 0],\n",
    "        cluster_points[:, 1],\n",
    "        cluster_points[:, 2],\n",
    "        s=50,  # Marker size\n",
    "        c=[mahalanobis_palette[i]],\n",
    "        label=f'Cluster {i}',\n",
    "        alpha=0.8,\n",
    "        edgecolors='w',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "# Mahalanobis plot\n",
    "ax2.set_title('Mahalanobis K-means Clusters', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "ax2.set_xlabel(f'PC1 ({explained_variance[0]:.1f}%)', fontsize=12, labelpad=10)\n",
    "ax2.set_ylabel(f'PC2 ({explained_variance[1]:.1f}%)', fontsize=12, labelpad=10)\n",
    "ax2.set_zlabel(f'PC3 ({explained_variance[2]:.1f}%)', fontsize=12, labelpad=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.xaxis.pane.fill = False\n",
    "ax2.yaxis.pane.fill = False\n",
    "ax2.zaxis.pane.fill = False\n",
    "ax2.xaxis.pane.set_edgecolor('w')\n",
    "ax2.yaxis.pane.set_edgecolor('w')\n",
    "ax2.zaxis.pane.set_edgecolor('w')\n",
    "ax2.view_init(elev=30, azim=45)  # Set viewing angle\n",
    "\n",
    "# Add legends with custom styling\n",
    "for ax in [ax1, ax2]:\n",
    "    legend = ax.legend(\n",
    "        title=\"Cluster Groups\",\n",
    "        title_fontsize=12,\n",
    "        fontsize=10,\n",
    "        loc='upper right',\n",
    "        bbox_to_anchor=(1.15, 0.9),\n",
    "        frameon=True,\n",
    "        facecolor='white',\n",
    "        edgecolor='#dddddd'\n",
    "    )\n",
    "    legend.get_frame().set_alpha(0.9)\n",
    "\n",
    "# title\n",
    "plt.suptitle('3D PCA Visualization of Clustering Results',\n",
    "             fontsize=20,\n",
    "             y=0.98,\n",
    "             fontweight='bold',\n",
    "             color='#333333')\n",
    "\n",
    "# explanatory text\n",
    "fig.text(0.5, 0.01,\n",
    "         f'Total explained variance: {sum(explained_variance[:3]):.1f}%',\n",
    "         ha='center',\n",
    "         fontsize=12,\n",
    "         color='#555555',\n",
    "         style='italic')\n",
    "\n",
    "# layout and save\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, bottom=0.1)\n",
    "plt.savefig('3d_clusters_visualization.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()\n",
    "\n",
    "# Create an additional 2D plot with density contours for better pattern visibility\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "fig.patch.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Function to add contours\n",
    "def add_density_contour(ax, x, y, color):\n",
    "    sns.kdeplot(x=x, y=y, ax=ax, levels=5, color=color, alpha=0.3, linewidths=1)\n",
    "\n",
    "# Euclidean clusters - 2D with contours\n",
    "for i in range(3):\n",
    "    cluster_points = X_train_pca[euclidean_clusters == i]\n",
    "    ax1.scatter(\n",
    "        cluster_points[:, 0],\n",
    "        cluster_points[:, 1],\n",
    "        s=60,\n",
    "        c=[euclidean_palette[i]],\n",
    "        label=f'Cluster {i}',\n",
    "        alpha=0.7,\n",
    "        edgecolors='w',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    add_density_contour(ax1, cluster_points[:, 0], cluster_points[:, 1], euclidean_palette[i])\n",
    "\n",
    "# Mahalanobis clusters - 2D with contours\n",
    "for i in range(3):\n",
    "    cluster_points = X_train_pca[mahalanobis_clusters == i]\n",
    "    ax2.scatter(\n",
    "        cluster_points[:, 0],\n",
    "        cluster_points[:, 1],\n",
    "        s=60,\n",
    "        c=[mahalanobis_palette[i]],\n",
    "        label=f'Cluster {i}',\n",
    "        alpha=0.7,\n",
    "        edgecolors='w',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    add_density_contour(ax2, cluster_points[:, 0], cluster_points[:, 1], mahalanobis_palette[i])\n",
    "\n",
    "# Customize 2D plots\n",
    "for i, ax in enumerate([ax1, ax2]):\n",
    "    title = 'Euclidean K-means Clusters' if i == 0 else 'Mahalanobis K-means Clusters'\n",
    "    ax.set_title(title, fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "    ax.set_xlabel(f'PC1 ({explained_variance[0]:.1f}%)', fontsize=12)\n",
    "    ax.set_ylabel(f'PC2 ({explained_variance[1]:.1f}%)', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Add legends with custom styling\n",
    "    legend = ax.legend(\n",
    "        title=\"Cluster Groups\",\n",
    "        title_fontsize=12,\n",
    "        fontsize=10,\n",
    "        loc='upper right',\n",
    "        frameon=True,\n",
    "        facecolor='white',\n",
    "        edgecolor='#dddddd'\n",
    "    )\n",
    "    legend.get_frame().set_alpha(0.9)\n",
    "\n",
    "plt.suptitle('2D PCA Visualization with Density Contours',\n",
    "             fontsize=20,\n",
    "             y=0.98,\n",
    "             fontweight='bold',\n",
    "             color='#333333')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.savefig('2d_clusters_with_contours.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "8TBV4heIGz7p",
    "outputId": "27e166d7-77f1-4b7e-9b6d-b5d537374bd0"
   },
   "outputs": [],
   "source": [
    "# Visualize target distribution within clusters\n",
    "plt.subplot(2, 2, 3)\n",
    "scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1],\n",
    "                     c=y_train.values.ravel(),\n",
    "                     cmap=LinearSegmentedColormap.from_list(\"custom_cmap\", ['#FF5E5B', '#D8D8D8', '#39A0ED']),\n",
    "                     s=50,\n",
    "                     alpha=0.7,\n",
    "                     edgecolors='w',\n",
    "                     linewidth=0.5)\n",
    "cbar = plt.colorbar(scatter, label='Rented Bike Count')\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "cbar.set_label('Rented Bike Count', fontsize=12, fontweight='bold', color='#333333')\n",
    "plt.title('Bike Count Distribution in PCA Space', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.xlabel(f'PC1 ({explained_variance[0]:.1f}%)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({explained_variance[1]:.1f}%)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "nR8gfumCG7o4",
    "outputId": "a6c8a314-342c-4ae3-cc32-dec3aa009536"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "feature_importance = baseline_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[-10:]  # Top 10 features\n",
    "\n",
    "\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(sorted_idx)))\n",
    "\n",
    "\n",
    "bars = plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx],\n",
    "         color=colors,\n",
    "         edgecolor='white',\n",
    "         linewidth=0.7)\n",
    "\n",
    "# Add value labels to the bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.002,\n",
    "             bar.get_y() + bar.get_height()/2,\n",
    "             f'{width:.4f}',\n",
    "             ha='left',\n",
    "             va='center',\n",
    "             fontsize=9,\n",
    "             color='#333333')\n",
    "\n",
    "# Customize appearance\n",
    "plt.yticks(range(len(sorted_idx)), X.columns[sorted_idx])\n",
    "plt.title('Top 10 Feature Importance', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.xlabel('Importance Score', fontsize=12, color='#333333')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "BJmBYBA0HOpj",
    "outputId": "75b61a9e-ab7f-4b40-c2b2-36bf7c4fd97a"
   },
   "outputs": [],
   "source": [
    "# Create a bar chart comparing the metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "fig.patch.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Define the colors from our palette\n",
    "colors = ['#39A0ED', '#FF5E5B', '#D8D8D8']  # Blue, Coral, Light Gray\n",
    "\n",
    "# RMSE comparison (lower is better)\n",
    "plt.subplot(1, 2, 1)\n",
    "models = ['Baseline', 'Euclidean\\nClustering', 'Mahalanobis\\nClustering']\n",
    "rmse_values = [baseline_metrics['rmse'], euclidean_metrics['rmse'], mahalanobis_metrics['rmse']]\n",
    "bars = plt.bar(models, rmse_values, color=colors,\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.title('RMSE Comparison (lower is better)', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('RMSE', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "min_rmse = min(rmse_values)\n",
    "plt.axhline(y=min_rmse, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, min_rmse-0.5, f'Best: {min_rmse:.2f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "5XRc09OPHdH4",
    "outputId": "46f9c8b5-10f6-4920-e66d-c9002971dd7a"
   },
   "outputs": [],
   "source": [
    "# R² comparison (higher is better)\n",
    "plt.subplot(1, 2, 2)\n",
    "r2_values = [baseline_metrics['r2'], euclidean_metrics['r2'], mahalanobis_metrics['r2']]\n",
    "bars = plt.bar(models, r2_values, color=colors,  # Using the same color palette\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Customize appearance\n",
    "plt.title('R² Comparison (higher is better)', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('R²', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add a horizontal line at the maximum R² for reference\n",
    "max_r2 = max(r2_values)\n",
    "plt.axhline(y=max_r2, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, max_r2-0.02, f'Best: {max_r2:.4f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('seoul_bike_metrics_comparison.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "zWQIwtG0dJSE",
    "outputId": "bfcb9b43-9a66-4338-e3d3-98ad6fe1ec5d"
   },
   "outputs": [],
   "source": [
    "# MAE comparison (lower is better)\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Calculate MAE for each model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "euclidean_mae = mean_absolute_error(y_test, euclidean_preds)\n",
    "mahalanobis_mae = mean_absolute_error(y_test, mahalanobis_preds)\n",
    "\n",
    "mae_values = [baseline_mae, euclidean_mae, mahalanobis_mae]\n",
    "bars = plt.bar(models, mae_values, color=colors,  # Using the same color palette\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.title('MAE Comparison (lower is better)', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('MAE', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add a horizontal line at the minimum MAE for reference\n",
    "min_mae = min(mae_values)\n",
    "plt.axhline(y=min_mae, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, min_mae-0.5, f'Best: {min_mae:.2f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('seoul_bike_metrics_comparison.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "9ENbW9vKdml0",
    "outputId": "bc410c79-d8c2-4476-89bf-8e1928d05308"
   },
   "outputs": [],
   "source": [
    "# Visualizing Cluster Analysis\n",
    "plt.figure(figsize=(18, 12))\n",
    "fig.patch.set_facecolor('#f8f9fa')\n",
    "\n",
    "\n",
    "euclidean_palette = ['#FF5E5B', '#D8D8D8', '#39A0ED']  # Coral, Light Gray, Blue\n",
    "\n",
    "# 1. Cluster Sizes - Top Left\n",
    "plt.subplot(2, 2, 1)\n",
    "euclidean_sizes = [sum(euclidean_clusters == i) for i in range(3)]\n",
    "mahalanobis_sizes = [sum(mahalanobis_clusters == i) for i in range(3)]\n",
    "\n",
    "x = np.arange(3)  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "rects1 = plt.bar(x - width/2, euclidean_sizes, width, label='Euclidean',\n",
    "                color=euclidean_palette, edgecolor='white', linewidth=0.8)\n",
    "rects2 = plt.bar(x + width/2, mahalanobis_sizes, width, label='Mahalanobis',\n",
    "                color=['#FFD166', '#06D6A0', '#118AB2'], edgecolor='white', linewidth=0.8)\n",
    "\n",
    "# Add labels and customize\n",
    "plt.xlabel('Cluster', fontsize=12, color='#333333')\n",
    "plt.ylabel('Number of Samples', fontsize=12, color='#333333')\n",
    "plt.title('Cluster Sizes Comparison', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.xticks(x, ['Cluster 0', 'Cluster 1', 'Cluster 2'])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for rect in rects1:\n",
    "    height = rect.get_height()\n",
    "    plt.annotate(f'{height}',\n",
    "                xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "for rect in rects2:\n",
    "    height = rect.get_height()\n",
    "    plt.annotate(f'{height}',\n",
    "                xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Average Bike Count per Cluster - Top Right\n",
    "plt.subplot(2, 2, 2)\n",
    "euclidean_avg_counts = []\n",
    "mahalanobis_avg_counts = []\n",
    "\n",
    "for i in range(3):\n",
    "    # Euclidean\n",
    "    cluster_indices = euclidean_clusters == i\n",
    "    if sum(cluster_indices) > 0:\n",
    "        avg_count = y_train.iloc[cluster_indices].values.mean()\n",
    "        euclidean_avg_counts.append(avg_count)\n",
    "    else:\n",
    "        euclidean_avg_counts.append(0)\n",
    "\n",
    "    # Mahalanobis\n",
    "    cluster_indices = mahalanobis_clusters == i\n",
    "    if sum(cluster_indices) > 0:\n",
    "        avg_count = y_train.iloc[cluster_indices].values.mean()\n",
    "        mahalanobis_avg_counts.append(avg_count)\n",
    "    else:\n",
    "        mahalanobis_avg_counts.append(0)\n",
    "\n",
    "rects1 = plt.bar(x - width/2, euclidean_avg_counts, width, label='Euclidean',\n",
    "                color=euclidean_palette, edgecolor='white', linewidth=0.8)\n",
    "rects2 = plt.bar(x + width/2, mahalanobis_avg_counts, width, label='Mahalanobis',\n",
    "                color=['#FFD166', '#06D6A0', '#118AB2'], edgecolor='white', linewidth=0.8)\n",
    "\n",
    "# Add labels and customize\n",
    "plt.xlabel('Cluster', fontsize=12, color='#333333')\n",
    "plt.ylabel('Average Bike Count', fontsize=12, color='#333333')\n",
    "plt.title('Average Bike Count per Cluster', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.xticks(x, ['Cluster 0', 'Cluster 1', 'Cluster 2'])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for rect in rects1:\n",
    "    height = rect.get_height()\n",
    "    plt.annotate(f'{height:.2f}',\n",
    "                xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "for rect in rects2:\n",
    "    height = rect.get_height()\n",
    "    plt.annotate(f'{height:.2f}',\n",
    "                xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_analysis_visualization.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0kQHk5KYfY8Y",
    "outputId": "0f7af64a-96be-472e-81d6-e7e497123005"
   },
   "outputs": [],
   "source": [
    "# 3. Distinctive Features Heatmap\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Get top distinctive features for each cluster\n",
    "all_top_features = set()\n",
    "feature_diff_values = {}\n",
    "for i in range(3):\n",
    "    cluster_indices = euclidean_clusters == i\n",
    "    if sum(cluster_indices) > 0:\n",
    "        cluster_data = X_train.iloc[cluster_indices]\n",
    "        feature_means = cluster_data.mean()\n",
    "        overall_means = X_train.mean()\n",
    "        feature_diff = abs(feature_means - overall_means)\n",
    "        top_features = feature_diff.nlargest(5).index\n",
    "\n",
    "        for feature in top_features:\n",
    "            all_top_features.add(feature)\n",
    "            key = (i, feature)\n",
    "            feature_diff_values[key] = (feature_means[feature] - overall_means[feature]) / overall_means[feature] * 100 if overall_means[feature] != 0 else 0\n",
    "\n",
    "# Create a matrix for the heatmap\n",
    "all_top_features = list(all_top_features)\n",
    "heatmap_data = np.zeros((len(all_top_features), 3))\n",
    "for i, feature in enumerate(all_top_features):\n",
    "    for cluster in range(3):\n",
    "        key = (cluster, feature)\n",
    "        if key in feature_diff_values:\n",
    "            heatmap_data[i, cluster] = feature_diff_values[key]\n",
    "\n",
    "# Create a custom colormap using the same colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Create a custom colormap that goes from blue (negative) to white (neutral) to red (positive)\n",
    "colors_for_map = []\n",
    "colors_for_map.append('#39A0ED')  # Blue for negative values\n",
    "colors_for_map.append('#FFFFFF')  # White for zero\n",
    "colors_for_map.append('#FF5E5B')  # Coral for positive values\n",
    "\n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_diverging', colors_for_map, N=256)\n",
    "\n",
    "# Increase the size of the heatmap by adjusting figure size\n",
    "plt.figure(figsize=(20, 12))  # Increase the figure size\n",
    "plt.subplot(1, 1, 1)  # Use the entire figure for the heatmap\n",
    "\n",
    "# Create the heatmap with the custom colormap\n",
    "im = plt.imshow(heatmap_data, cmap=custom_cmap, aspect='auto', vmin=-400, vmax=400)\n",
    "cbar = plt.colorbar(im, label='% Difference from Overall Mean', fraction=0.046, pad=0.04)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_label('% Difference from Overall Mean', fontsize=14, color='#333333')\n",
    "\n",
    "# Add labels with larger font sizes\n",
    "plt.yticks(range(len(all_top_features)), all_top_features, fontsize=12)\n",
    "plt.xticks(range(3), ['Cluster 0', 'Cluster 1', 'Cluster 2'], fontsize=14)\n",
    "plt.title('Distinctive Features by Cluster (Euclidean)', fontsize=18, pad=20, fontweight='bold', color='#333333')\n",
    "\n",
    "# Add text annotations with larger font size\n",
    "for i in range(len(all_top_features)):\n",
    "    for j in range(3):\n",
    "        value = heatmap_data[i, j]\n",
    "        # Use white text for extreme values, black for moderate values\n",
    "        text_color = 'white' if abs(value) > 100 else 'black'\n",
    "        plt.text(j, i, f'{value:.1f}%', ha='center', va='center',\n",
    "                color=text_color, fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distinctive_features_heatmap.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1P4XN1KLHiDM",
    "outputId": "80969134-a90d-4b86-e623-615732fb5663"
   },
   "outputs": [],
   "source": [
    "# Additional analysis: Cluster sizes and characteristics\n",
    "print(\"\\n=== CLUSTER ANALYSIS ===\")\n",
    "print(\"\\nEuclidean Cluster Sizes:\")\n",
    "for i in range(3):\n",
    "    print(f\"Cluster {i}: {sum(euclidean_clusters == i)} samples\")\n",
    "\n",
    "print(\"\\nMahalanobis Cluster Sizes:\")\n",
    "for i in range(3):\n",
    "    print(f\"Cluster {i}: {sum(mahalanobis_clusters == i)} samples\")\n",
    "\n",
    "# Average bike count per cluster\n",
    "print(\"\\nAverage Bike Count per Cluster:\")\n",
    "print(\"\\nEuclidean Clusters:\")\n",
    "for i in range(3):\n",
    "    cluster_indices = euclidean_clusters == i\n",
    "    if sum(cluster_indices) > 0:\n",
    "        avg_count = y_train.iloc[cluster_indices].values.mean()\n",
    "        print(f\"Cluster {i}: {avg_count:.2f}\")\n",
    "\n",
    "print(\"\\nMahalanobis Clusters:\")\n",
    "for i in range(3):\n",
    "    cluster_indices = mahalanobis_clusters == i\n",
    "    if sum(cluster_indices) > 0:\n",
    "        avg_count = y_train.iloc[cluster_indices].values.mean()\n",
    "        print(f\"Cluster {i}: {avg_count:.2f}\")\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "print(\"\\nCluster Characteristics (Average Feature Values):\")\n",
    "print(\"\\nEuclidean Clusters:\")\n",
    "for i in range(3):\n",
    "    cluster_indices = euclidean_clusters == i\n",
    "    if sum(cluster_indices) > 0:\n",
    "        cluster_data = X_train.iloc[cluster_indices]\n",
    "        # Get top 5 most distinctive features (largest difference from overall mean)\n",
    "        feature_means = cluster_data.mean()\n",
    "        overall_means = X_train.mean()\n",
    "        feature_diff = abs(feature_means - overall_means)\n",
    "        top_features = feature_diff.nlargest(5).index\n",
    "\n",
    "        print(f\"\\nCluster {i} distinctive features:\")\n",
    "        for feature in top_features:\n",
    "            print(f\"{feature}: {feature_means[feature]:.2f} (overall: {overall_means[feature]:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yFkCZCB2WsRp",
    "x6hwTLKoW-d_",
    "QhRPHn3wXK2V"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
