{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-M_rHR3pgJgu"
   },
   "source": [
    "# **Experiment Objectives and Hypotheses**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic2x8GRggQxg"
   },
   "source": [
    "## **Hypotheses:**\n",
    "\n",
    "\n",
    "\n",
    "*   H1: Training separate regression models for each of the three identified clusters will improve performance (RMSE, MAE, R²) over a single unified model.\n",
    "\n",
    "* H2: Clustering using Mahalanobis distance will yield more effective segments for modeling than Euclidean distance.\n",
    "\n",
    "* H3: Training separate models for each season (Spring, Summer, Fall, Winter) will improve predictive accuracy by capturing seasonal variation in bike rental behavior.\n",
    "\n",
    "* H4: Neural Network outperforms our baseline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Research Questions:**\n",
    "* RQ1: Does cluster-based modeling improve regression performance compared to a single global model?\n",
    "\n",
    "* RQ2: Which clustering distance metric (Mahalanobis vs. Euclidean) produces better downstream model performance?\n",
    "\n",
    "* RQ3: Can season-specific models capture trends in bike demand better than a unified model?\n",
    "\n",
    "* RQ4: Among XGBoost, neural networks, and cluster-wise regressors, which method yields the best performance?\n",
    "\n",
    "\n",
    "\n",
    "## **Experimental Design:**\n",
    "### Baseline Model:\n",
    "XGBoost Regressor\n",
    "### Target Variable:\n",
    "Rented Bike Count\n",
    "### Dataset:\n",
    "Seoul Bike Sharing Demand\n",
    "### Data Split:\n",
    "80% training, 20% testing\n",
    "\n",
    "### Method:\n",
    "[Insert a brief description of your method here]\n",
    "\n",
    "### Visualization:\n",
    "\n",
    "[Insert if applicable]\n",
    "\n",
    "\n",
    "### Performance Metrics:\n",
    "RMSE, MAE, and R²\n",
    "\n",
    "## **Expected Outcomes:**\n",
    "\n",
    "\n",
    "*  [Insert expectations from experiments]\n",
    "*  [Insert expectations from experiments]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFkCZCB2WsRp"
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NalgnChvpx1y"
   },
   "source": [
    "include additional libraries for development in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HsjQHFg_0Yz",
    "outputId": "87ebddb7-7702-496f-f01c-86a9746160f1"
   },
   "outputs": [],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nfbZTdB_w0K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "mpl.rcParams['axes.edgecolor'] = '#333333'\n",
    "mpl.rcParams['axes.linewidth'] = 0.8\n",
    "mpl.rcParams['xtick.color'] = '#333333'\n",
    "mpl.rcParams['ytick.color'] = '#333333'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6hwTLKoW-d_"
   },
   "source": [
    "# **Import Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sGCzaCo_7EK"
   },
   "outputs": [],
   "source": [
    "# Fetch Seoul Bike Sharing Demand dataset from UCI ML Repository\n",
    "seoul_bike_sharing_demand = fetch_ucirepo(id=560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2TCiohL_-VU"
   },
   "outputs": [],
   "source": [
    "# Data (as pandas dataframes)\n",
    "X_original = seoul_bike_sharing_demand.data.features\n",
    "y_original = seoul_bike_sharing_demand.data.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kceoOc1_Cgpp",
    "outputId": "01eb29c1-db0e-471e-c5f0-443132789cbb"
   },
   "outputs": [],
   "source": [
    "# Print dataset information\n",
    "print(\"Dataset Metadata:\")\n",
    "print(seoul_bike_sharing_demand.metadata)\n",
    "print(\"\\nVariable Information:\")\n",
    "print(seoul_bike_sharing_demand.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXhCwGVOBWIE",
    "outputId": "6a140342-dee9-4818-e4b0-edd5f08fbff9"
   },
   "outputs": [],
   "source": [
    "# Examine feature information\n",
    "print(\"\\nOriginal feature columns:\")\n",
    "print(X_original.columns.tolist())\n",
    "print(\"\\nOriginal target variable:\")\n",
    "print(y_original.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhRPHn3wXK2V"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMA5KVPJqNEH"
   },
   "source": [
    "**Data Preprocessing Steps**\n",
    "\n",
    "\n",
    "\n",
    "*   Makes \"Rented Bike Count\" new target. Move former target from dataset api to features\n",
    "*   Check for missing values\n",
    "*  Transform date column to date time\n",
    "*  Transform categorical features using one hot encoding\n",
    "* Split dataset into test (80%) and training (20%)\n",
    "* Create heatmap to identify correlated features\n",
    "* Compute VIF (Variance Inflation Factor) to measure multicolinearity\n",
    "* Define function to remove colinear features using a 0.8 Colinearity threshold\n",
    "* Compare VIF before and after removal\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wY0aMkBoBtrX"
   },
   "outputs": [],
   "source": [
    "# Make 'Rented Bike Count' the new target if it exists\n",
    "\n",
    "if 'Rented Bike Count' in X_original.columns:\n",
    "    # Make 'Rented Bike Count' the new target\n",
    "    y = X_original[['Rented Bike Count']]\n",
    "    # Remove 'Rented Bike Count' from features\n",
    "    X = X_original.drop('Rented Bike Count', axis=1)\n",
    "    # Add original target to features\n",
    "    X = pd.concat([X, y_original], axis=1)\n",
    "\n",
    "else:\n",
    "    # If 'Rented Bike Count' is already the target, just confirm\n",
    "    print(\"'Rented Bike Count' is already the target variable.\")\n",
    "    y = y_original\n",
    "    X = X_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpuLenjDAB_4",
    "outputId": "186b7f4f-f22b-4237-9eb0-2f208333c174"
   },
   "outputs": [],
   "source": [
    "print(\"\\nNew feature columns:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"\\nNew target variable:\")\n",
    "print(y.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELZ5f9cmAE0W",
    "outputId": "e87a2dd3-abff-4b2f-a74d-7cbbb896900f"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in features:\")\n",
    "print(X.isnull().sum())\n",
    "print(\"\\nMissing values in target:\")\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0ZypEjpAjqO"
   },
   "outputs": [],
   "source": [
    "# 1. Convert date column to datetime and extract useful components. date format is DD/MM/YYYY\n",
    "if 'Date' in X.columns:\n",
    "    # Specify the correct date format as DD/MM/YYYY\n",
    "    X['Date'] = pd.to_datetime(X['Date'], format='%d/%m/%Y')\n",
    "    X['Year'] = X['Date'].dt.year\n",
    "    X['Month'] = X['Date'].dt.month\n",
    "    X['Day'] = X['Date'].dt.day\n",
    "    X['DayOfWeek'] = X['Date'].dt.dayofweek\n",
    "    X = X.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqzBrisvEfmV"
   },
   "outputs": [],
   "source": [
    "# 2. Convert categorical features to numeric using one-hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "KyW3-YG4QpIi",
    "outputId": "aaf03e76-e9e4-45d4-9a01-890219bc799d"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUXcIclqElR5"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684
    },
    "id": "fMNS9RuVTF7w",
    "outputId": "425960a4-cd90-4d89-c2c5-ce819a3b8723"
   },
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "\n",
    "correlation_matrix = X_train_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Use the calculated correlation matrix in the heatmap\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, fmt='.2f')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE8rz2U1Ueay"
   },
   "outputs": [],
   "source": [
    "# Function to calculate VIF (Variance Inflation Factor)\n",
    "def calculate_vif(X):\n",
    "    # Select only numeric columns to avoid issues with non-numeric data types\n",
    "    X_numeric = X.select_dtypes(include=np.number)\n",
    "    # Drop columns with any non-finite values if they exist (though the type error suggests something else)\n",
    "    # X_numeric = X_numeric.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "    # Check if there are still columns left\n",
    "    if X_numeric.shape[1] == 0:\n",
    "        print(\"Warning: No numeric columns found for VIF calculation.\")\n",
    "        return pd.DataFrame(columns=[\"feature\", \"VIF\"])\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X_numeric.columns\n",
    "    # Ensure the values passed to variance_inflation_factor are finite and the correct type\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_numeric.values, i) for i in range(X_numeric.shape[1])]\n",
    "    return vif_data.sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "# Function to remove collinear features based on correlation threshold\n",
    "def remove_collinear_features(X, threshold=0.8):\n",
    "    # Select only numeric columns before calculating correlation\n",
    "    X_numeric = X.select_dtypes(include=np.number)\n",
    "\n",
    "    corr_matrix = X_numeric.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Plot correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=False,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    print(f\"Dropping {len(to_drop)} collinear features: {', '.join(to_drop)}\")\n",
    "\n",
    "    # Return dataframe with collinear features removed (applied to original X)\n",
    "    return X.drop(to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ROXG-v8eVN_k",
    "outputId": "0465025f-5402-40f1-bf34-272e6fa6f1ba"
   },
   "outputs": [],
   "source": [
    "# Calculate VIF before feature removal\n",
    "print(\"Variance Inflation Factors before colinearity removal:\")\n",
    "# Ensure only numeric columns are passed to calculate_vif\n",
    "X_train_numeric = X_train.select_dtypes(include=np.number)\n",
    "vif_before = calculate_vif(X_train_numeric)\n",
    "print(vif_before.head(10))  # Show top 10 highest VIF values\n",
    "\n",
    "# Remove collinear features\n",
    "# The remove_collinear_features function now handles numeric selection internally for correlation\n",
    "X_train_filtered = remove_collinear_features(X_train, threshold=0.8)\n",
    "# Ensure test set has the same columns as the filtered training set\n",
    "X_test_filtered = X_test[X_train_filtered.columns]\n",
    "\n",
    "# Calculate VIF after feature removal\n",
    "print(\"\\nVariance Inflation Factors after collinearity removal:\")\n",
    "# Ensure only numeric columns are passed to calculate_vif\n",
    "X_train_filtered_numeric = X_train_filtered.select_dtypes(include=np.number)\n",
    "vif_after = calculate_vif(X_train_filtered_numeric)\n",
    "print(vif_after.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8x5rCrwUSxP"
   },
   "outputs": [],
   "source": [
    "# Standardize the filtered features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
    "X_test_scaled = scaler.transform(X_test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qthduIVYs-Lp"
   },
   "source": [
    "# **Baseline Model Development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aT6nh24-tDCV",
    "outputId": "7d0a4eb0-22ad-479a-8344-6076afd16535"
   },
   "outputs": [],
   "source": [
    "# Baseline XGBoost model\n",
    "print(\"Training baseline XGBoost regression model...\")\n",
    "baseline_model = xgb.XGBRegressor(random_state=42)\n",
    "baseline_model.fit(X_train_scaled, y_train.values.ravel())\n",
    "baseline_preds = baseline_model.predict(X_test_scaled)\n",
    "baseline_metrics = {\n",
    "    'rmse': math.sqrt(mean_squared_error(y_test, baseline_preds)),\n",
    "    'r2': r2_score(y_test, baseline_preds)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfs7TqaNXbmM"
   },
   "source": [
    "# **Model Development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKoSCnIHtypv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjdW7iiSgAOi"
   },
   "source": [
    "# **Evaluation and Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "BJmBYBA0HOpj",
    "outputId": "75b61a9e-ab7f-4b40-c2b2-36bf7c4fd97a"
   },
   "outputs": [],
   "source": [
    "# Create a bar chart comparing the metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "fig.patch.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Define the colors from our palette\n",
    "colors = ['#39A0ED', '#FF5E5B', '#D8D8D8']  # Blue, Coral, Light Gray\n",
    "\n",
    "# RMSE comparison (lower is better)\n",
    "plt.subplot(1, 2, 1)\n",
    "models = ['Baseline', 'Model_developed_1', 'Model_developed_2']\n",
    "rmse_values = [baseline_metrics['rmse'], Model_developed_1['rmse'], Model_developed_2['rmse']]\n",
    "bars = plt.bar(models, rmse_values, color=colors,\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.title('RMSE Comparison (lower is better)', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('RMSE', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "min_rmse = min(rmse_values)\n",
    "plt.axhline(y=min_rmse, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, min_rmse-0.5, f'Best: {min_rmse:.2f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "5XRc09OPHdH4",
    "outputId": "46f9c8b5-10f6-4920-e66d-c9002971dd7a"
   },
   "outputs": [],
   "source": [
    "# R² comparison (higher is better)\n",
    "plt.subplot(1, 2, 2)\n",
    "r2_values = [baseline_metrics['r2'], Model_developed_1['r2'], Model_developed_2['r2']]\n",
    "bars = plt.bar(models, r2_values, color=colors,  # Using the same color palette\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Customize appearance\n",
    "plt.title('R² Comparison (higher is better)', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('R²', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add a horizontal line at the maximum R² for reference\n",
    "max_r2 = max(r2_values)\n",
    "plt.axhline(y=max_r2, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, max_r2-0.02, f'Best: {max_r2:.4f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('seoul_bike_metrics_comparison.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "zWQIwtG0dJSE",
    "outputId": "bfcb9b43-9a66-4338-e3d3-98ad6fe1ec5d"
   },
   "outputs": [],
   "source": [
    "# MAE comparison (lower is better)\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Calculate MAE for each model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
    "Model_developed_1_mae = mean_absolute_error(y_test, Model_developed_1_preds)\n",
    "Model_developed_2_mae = mean_absolute_error(y_test, Model_developed_2_preds)\n",
    "\n",
    "mae_values = [baseline_mae, Model_developed_1, Model_developed_2]\n",
    "bars = plt.bar(models, mae_values, color=colors,  # Using the same color palette\n",
    "               edgecolor='white', linewidth=0.8, width=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.title('MAE Comparison (lower is better)', fontsize=16, pad=20, fontweight='bold', color='#333333')\n",
    "plt.ylabel('MAE', fontsize=12, color='#333333')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add a horizontal line at the minimum MAE for reference\n",
    "min_mae = min(mae_values)\n",
    "plt.axhline(y=min_mae, color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.text(len(models)-1, min_mae-0.5, f'Best: {min_mae:.2f}',\n",
    "         ha='right', va='top', color='#333333', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('seoul_bike_metrics_comparison.png', dpi=300, bbox_inches='tight', facecolor='#f8f9fa')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yFkCZCB2WsRp",
    "x6hwTLKoW-d_",
    "QhRPHn3wXK2V"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
