{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploring Gradient Boosted Decision Trees\n",
    "Structure and rough draft for a notebook investigating XGBoost and PME (later)\n",
    "\n",
    "\"XGBoost stands for ‚ÄúExtreme Gradient Boosting‚Äù, where the term ‚ÄúGradient Boosting‚Äù originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman.\"\n",
    "\n",
    "from [Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/stable/tutorials/model.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "seoul_bike_sharing_demand = fetch_ucirepo(id=560)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = seoul_bike_sharing_demand.data.features\n",
    "y = seoul_bike_sharing_demand.data.targets\n",
    "\n",
    "# metadata\n",
    "print(seoul_bike_sharing_demand.metadata)\n",
    "\n",
    "# variable information\n",
    "print(seoul_bike_sharing_demand.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor, plot_importance, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recombining Features and Target for Analysis\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y') # Changed the format string to match the actual format\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Weekday'] = df['Date'].dt.weekday\n",
    "df.drop(columns=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables\n",
    "df = pd.get_dummies(df, columns=['Seasons', 'Holiday', 'Functioning Day'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing numerical values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.drop('Rented Bike Count')\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['Rented Bike Count'])\n",
    "y = df['Rented Bike Count']\n",
    "\n",
    "# Split into train (60%), eval (20%), test (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_eval, X_test, y_eval, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Exploring XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "eval_set = [(X_train, y_train), (X_eval, y_eval)]\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=eval_set, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## XGBoost has a dedicated plotting library\n",
    "\n",
    "[üìò Documentation](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.plot_importance)\n",
    "\n",
    "XGBoost provides a built-in function to **plot the importance of each feature** based on various criteria. The `importance_type` parameter allows you to choose how importance is measured:\n",
    "\n",
    "- **`weight`**: The number of times a feature appears in a tree.\n",
    "- **`gain`**: The average gain of splits which use the feature.\n",
    "- **`cover`**: The average coverage of splits which use the feature, where _coverage_ is defined as the number of samples affected by the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(model, importance_type='gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(model, importance_type='weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(model, importance_type='cover')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred_xgb = model.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"\\nXGBoost Metrics:\")\n",
    "print(f\"MAE: {mae_xgb:.2f}\")\n",
    "print(f\"R¬≤: {r2_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_xgb, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('XGBoost: Predicted vs Actual Values')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evals_result()\n",
    "train_loss = results['validation_0']['rmse']\n",
    "eval_loss = results['validation_1']['rmse']\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Train RMSE')\n",
    "plt.plot(eval_loss, label='Validation RMSE')\n",
    "plt.title('XGBoost RMSE Over Boosting Rounds')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.abs(np.array(train_loss) - np.array(eval_loss)), label='Train-Eval RMSE Gap')\n",
    "plt.title('Generalization Gap Over Time')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('Gap')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
